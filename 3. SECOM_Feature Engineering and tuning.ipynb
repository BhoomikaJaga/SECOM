{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafd661c",
   "metadata": {},
   "source": [
    "# AIM:\n",
    "\n",
    "### Aim for this script is to build the model for Semi conductor industry producing wafers with 591 sensors.\n",
    "### To build the model, CRISP DM iterative methodology is being followed with Business Understading, Data understadning, Data preparation, Model building, Evaluation and deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8a16ba",
   "metadata": {},
   "source": [
    "### DATA sets: SECOM and Label data has been used for the script - 1567 wafers\n",
    "### SECOM: contains 591 features/ Predictors which are basically sensors and Label data contains Timestamp entry for wafer generation and Target variable with 1(fail) and -1(pass) entries "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b729df40",
   "metadata": {},
   "source": [
    "# 1. Data understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a51f5a3",
   "metadata": {},
   "source": [
    "## Load Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdacf5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data understanding\n",
    "\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# data preparation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import KNNImputer from sklearn.experimental \n",
    "import enable_iterative_imputer from sklearn.impute \n",
    "import IterativeImputer, SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer \n",
    "import matplotlib.pyplot as plt\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE, ADASYN,RandomOverSampler \n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# modeling\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from boruta import BorutaPy\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA \n",
    "from boruta import BorutaPy\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "from sklearn.impute import KNNImputer, SimpleImputer, IterativeImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# evaluation\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc5881",
   "metadata": {},
   "source": [
    "## Load datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b87875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the file path with the actual path to your file\n",
    "file_path = \"C:/Users/DhruviJayPatel/Documents/SECOMProject/secom_data/secom.data\"\n",
    "# Read the data into a DataFrame\n",
    "secom = pd.read_csv(file_path, header=None, sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3ac584",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_2 =\"C:/Users/DhruviJayPatel/Documents/SECOMProject/secom_data/secom_labels.data\"\n",
    "label = pd.read_csv(file_path_2, header=None, sep=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf65ad0",
   "metadata": {},
   "source": [
    "## merge features and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395701c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([secom, label], axis=1) \n",
    "print(merged_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b976fc23",
   "metadata": {},
   "source": [
    "## name predictors for further analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac562a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Assuming secom DataFrame with 591 features\n",
    "\n",
    "# Generate new column names using a list comprehension\n",
    "\n",
    "new_column_names = [f'feature{i}' for i in range(1, 593)]\n",
    "\n",
    "# Rename columns in the DataFrame \n",
    "merged_df.columns = new_column_names\n",
    "\n",
    "# Print the DataFrame to verify the changes\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd0a493",
   "metadata": {},
   "source": [
    "#### To understand the data comprehensively, several analyses were conducted based on following methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88ab07d",
   "metadata": {},
   "source": [
    "## Explotary Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766e667e",
   "metadata": {},
   "source": [
    "#### EDA is a crucial step in the data analysis process that involves summarizing and visualizing data to understand its main characteristics\n",
    "#### It helps in understanding the structure, patterns, and anomalies in data, which is essential for accurate and effective analysis. It aids in preparing data for modeling, forming hypotheses, and ensuring that the assumptions of statistical models are met. Additionally, EDA facilitates clear communication of data insights, supporting data-driven decision-making"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80255ddf",
   "metadata": {},
   "source": [
    "## Histogram of Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a58784",
   "metadata": {},
   "source": [
    "#### Features with a high number of missing values can reduce the model's performance, as they do not contribute meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d89a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each feature\n",
    "missing_percentage = (secom.isnull().sum() / len(secom)) * 100\n",
    "\n",
    "# Plot histogram of percentage of missing values \n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(missing_percentage, bins=50, color='skyblue', \n",
    "edgecolor='black')\n",
    "plt.title('Histogram of Percentage of Missing Values') \n",
    "plt.xlabel('Percentage of Missing Values') \n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b821867",
   "metadata": {},
   "source": [
    "## histogram of Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d395425",
   "metadata": {},
   "source": [
    "#### Like missing values, features with high volatility (numerous missing values) may not add significant value to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate variance for each feature \n",
    "\n",
    "variances = secom.var()\n",
    "\n",
    "# Generate bin edges with intervals of 0.5 starting from 0 \n",
    "# Plot histogram of variances\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.hist(variances, bins=50, color='skyblue', edgecolor='black') #\n",
    "Use bin edges generated above \n",
    "plt.title('Histogram of Feature Variances') \n",
    "plt.xlabel('Variance') \n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True) \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a3dd8",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c2f2ce",
   "metadata": {},
   "source": [
    "#### Understanding the correlation structure helps in identifying multicollinearity and se-lecting the most informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e27e8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = secom.corr()\n",
    "# Plot the heatmap \n",
    "plt.figure(figsize=(20, 15))\n",
    "sns.heatmap(correlation_matrix, cmap=\"coolwarm\", vmin=-1, \n",
    "vmax=1, center=0, annot=False)\n",
    "plt.title('Correlation Heatmap of 592 Variables') \n",
    "plt.xlabel('Features') # Add x-axis label \n",
    "plt.ylabel('Features') # Add y-axis label \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea09d3",
   "metadata": {},
   "source": [
    "## Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82594d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_rows = secom.duplicated()\n",
    "# Count the number of duplicate rows \n",
    "num_duplicates = duplicate_rows.sum()\n",
    "print(\"Number of duplicate rows:\", num_duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fcff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duplicate_features = sum(secom.T.duplicated())\n",
    "# Print the total number of duplicate features \n",
    "\n",
    "print(\"Total number of duplicate features:\",total_duplicate_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f95b7",
   "metadata": {},
   "source": [
    "## Pareto chart of Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994bde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values for each column \n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "# Sort the columns based on the percentage of missing values in \n",
    "descending order\n",
    "sorted_indices = \n",
    "missing_percentage.sort_values(ascending=False).index\n",
    "# Create a histogram of the missing values \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(missing_percentage, bins=20, color='skyblue', \n",
    "edgecolor='black')\n",
    "plt.xlabel('Percentage of Missing Values') \n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Missing Values')\n",
    "# Plot Pareto line on the same graph \n",
    "cumulative_percentage = \n",
    "(missing_percentage[sorted_indices].cumsum() / \n",
    "missing_percentage.sum() * 100).values \n",
    "plt.twinx()\n",
    "plt.xlim(0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6161f7c0",
   "metadata": {},
   "source": [
    "## Threshold Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755bfa23",
   "metadata": {},
   "source": [
    "#### features with higher missing values does not contribute to the model building process.\n",
    "#### define one threshold and remove the features which are above the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9bc9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Define the threshold for missing values (e.g., 10%) \n",
    "threshold = 60\n",
    "# Calculate the percentage of missing values for each column in \n",
    "the training set\n",
    "train_missing_percentage = (X_train.isna().mean() * 100).round(2)\n",
    "# Calculate the percentage of missing values for each column in \n",
    "the testing set\n",
    "test_missing_percentage = (X_test.isna().mean() * 100).round(2)\n",
    "# Plot histogram of missing value percentages for the training set \n",
    "plt.figure(figsize=(12, 6))\n",
    "train_hist, train_bins, _ = plt.hist(train_missing_percentage, \n",
    "bins=10, range=(40, 100), color='skyblue', edgecolor='black') \n",
    "plt.axvline(x=threshold, color='red', linestyle='--', label='Threshold')\n",
    "# Add annotations for each bar in the training histogram \n",
    "for i, freq in enumerate(train_hist):\n",
    "plt.text(train_bins[i], freq, str(int(freq)), ha='center', va='bottom')\n",
    "plt.title('Histogram of Missing Value Percentages in Training Set') \n",
    "plt.xlabel('Percentage of Missing Values')\n",
    "plt.ylabel('Frequency') \n",
    "plt.legend() \n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "# Plot histogram of missing value percentages for the testing set \n",
    "plt.figure(figsize=(12, 6))\n",
    "test_hist, test_bins, _ = plt.hist(test_missing_percentage, bins=10, \n",
    "range=(40, 100), color='salmon', edgecolor='black') \n",
    "plt.axvline(x=threshold, color='red', linestyle='--', label='Threshold')\n",
    "# Add annotations for each bar in the testing histogram \n",
    "for i, freq in enumerate(test_hist):\n",
    "plt.text(test_bins[i], freq, str(int(freq)), ha='center', va='bottom'\n",
    "plt.title('Histogram of Missing Value Percentages in Testing Set') \n",
    "plt.xlabel('Percentage of Missing Values')\n",
    "plt.ylabel('Frequency') \n",
    "plt.legend() \n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a563c",
   "metadata": {},
   "source": [
    "## outlier anaylsis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12733833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate z-scores for each feature\n",
    "z_scores = (X_train - X_train.mean()) / X_train.std() \n",
    "z_scores_test = (X_test - X_test.mean()) / X_test.std()\n",
    "# Define threshold for identifying outliers \n",
    "threshold = 3\n",
    "# Find outliers for each feature\n",
    "outliers = np.abs(z_scores) > threshold \n",
    "outliers_test = np.abs(z_scores_test) > threshold\n",
    "# Count number of outliers for each feature \n",
    "num_outliers = np.sum(outliers, axis=0)\n",
    "# Calculate percentage of outliers for each feature \n",
    "percentage_outliers = (num_outliers / len(X_train)) * 100\n",
    "import matplotlib.pyplot as plt \n",
    "# Define bin edges\n",
    "bin_edges = [0, 0.001] + list(range(1, 6))\n",
    "# Plot histogram \n",
    "plt.figure(figsize=(10, 6))\n",
    "hist = plt.hist(percentage_outliers, bins=bin_edges, \n",
    "color='skyblue', edgecolor='black')\n",
    "plt.title(\"Histogram of Percentages of Outliers in Each Column\") \n",
    "plt.xlabel(\"Percentage of Outliers\")\n",
    "plt.ylabel(\"Frequency\") \n",
    "plt.xticks(bin_edges) \n",
    "for bar in hist[2]:\n",
    "height = int(bar.get_height())\n",
    "plt.text(bar.get_x() + bar.get_width() / 2, height, height, \n",
    "ha='center', va='bottom')\n",
    "plt.grid(axis='y') \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63106610",
   "metadata": {},
   "source": [
    "# 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a069b5",
   "metadata": {},
   "source": [
    "## Splitting - 75-25 split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918ec990",
   "metadata": {},
   "source": [
    "#### Dataset is having 1:14 ratio of pass and fail cases, which is indicating highly imbalanced dataset and to ensure that same ratio should be maintained after split in train and test data and avoid biasness, stratified sampling can be used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = merged_df['feature591'] # Replace 'target_column_name' with \n",
    "the name of your target column\n",
    "# Dropping the target variable from the dataframe to get only the \n",
    "features\n",
    "X = merged_df.drop('feature591', axis=1)\n",
    "# Split pass cases into training and testing sets while preserving \n",
    "the distribution\n",
    "X_pass_train, X_pass_test, y_pass_train, y_pass_test = \n",
    "train_test_split(\n",
    "X[y == -1], y[y == -1], test_size=0.25, random_state=42, \n",
    "stratify=y[y == -1])\n",
    "# Split fail cases into training and testing sets while preserving the \n",
    "distribution\n",
    "X_fail_train, X_fail_test, y_fail_train, y_fail_test = train_test_split( \n",
    "X[y == 1], y[y == 1], test_size=0.25, random_state=42, stratify=y[y\n",
    "== 1])\n",
    "# Concatenate the pass and fail cases in training and testing sets \n",
    "X_train = pd.concat([X_pass_train, X_fail_train])\n",
    "y_train = pd.concat([y_pass_train, y_fail_train]) \n",
    "X_test = pd.concat([X_pass_test, X_fail_test]) \n",
    "y_test = pd.concat([y_pass_test, y_fail_test])\n",
    "print(\"Training set - Features:\", X_train.shape, \"Labels:\", \n",
    "y_train.shape)\n",
    "print(\"Testing set - Features:\", X_test.shape, \"Labels:\", \n",
    "y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e599cf21",
   "metadata": {},
   "source": [
    "## Pass and fail proportion in original, train and testdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6ef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_original_proportion = (merged_df['feature591'] == -1).mean() \n",
    "fail_original_proportion = (merged_df['feature591'] == 1).mean()\n",
    "print(\"Original Dataset:\")\n",
    "print(\"Pass cases proportion:\", pass_original_proportion) \n",
    "print(\"Fail cases proportion:\", fail_original_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea76990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the proportion of pass and fail cases in the training set \n",
    "pass_train_proportion = (y_train == -1).mean() \n",
    "fail_train_proportion = (y_train == 1).mean()\n",
    "# Calculate the proportion of pass and fail cases in the testing set \n",
    "pass_test_proportion = (y_test == -1).mean()\n",
    "fail_test_proportion = (y_test == 1).mean()\n",
    "print(\"Training Set:\")\n",
    "print(\"Pass cases proportion:\", pass_train_proportion) \n",
    "print(\"Fail cases proportion:\", fail_train_proportion) \n",
    "print(\"\\nTesting Set:\")\n",
    "print(\"Pass cases proportion:\", pass_test_proportion) \n",
    "print(\"Fail cases proportion:\", fail_test_proportion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ba19b",
   "metadata": {},
   "source": [
    "# Rough feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f6bf3c",
   "metadata": {},
   "source": [
    "## removal zero variance features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad553ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "variance = X_train.var()\n",
    "columns_to_keep = variance[variance != 0].index \n",
    "X_train = X_train[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fd2f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "X_train = X_train[columns_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438bb7c",
   "metadata": {},
   "source": [
    "## Outlier handing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e72df6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \n",
    "# Replace outliers with NaNs in X_train DataFrame \n",
    "X_train = X_train.mask(outliers)\n",
    "X_test= X_test.mask(outliers_test)\n",
    "# Print the first few rows of the cleaned DataFrame to verify \n",
    "print(\"First few rows of X_train after replacing outliers with NaNs:\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4bc936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3s boundaries\n",
    "\n",
    "# Function to replace outliers based on 3 standard deviations \n",
    "and count them, excluding specified columns\n",
    "def replace_and_count_outliers(df, exclude_columns=[]): \n",
    "outlier_counts = {}\n",
    "for column in df.columns:\n",
    "if column not in exclude_columns: \n",
    "mean = df[column].mean()\n",
    "std = df[column].std() \n",
    "lower_bound = mean - 3 * std \n",
    "upper_bound = mean + 3 * std\n",
    "\n",
    "# Count outliers before replacing\n",
    "outliers = ((df[column] < lower_bound) | (df[column] > \n",
    "upper_bound)).sum()\n",
    "outlier_counts[column] = outliers\n",
    "# Replace outliers\n",
    "df[column] = np.where(df[column] < lower_bound, \n",
    "lower_bound, df[column])\n",
    "df[column] = np.where(df[column] > upper_bound, \n",
    "upper_bound, df[column])\n",
    "return df, outlier_counts\n",
    "\n",
    "# Replace outliers in the original training set and count them \n",
    "X_test, outliers_count = replace_and_count_outliers(X_test)\n",
    "# Convert to a DataFrame for better readability \n",
    "outliers_df = pd.DataFrame.from_dict(outliers_count, \n",
    "orient='index', columns=['Outliers Count'])\n",
    "# Print the outlier counts after replacement \n",
    "print(outliers_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b092602",
   "metadata": {},
   "source": [
    "## Missing Value Removal with threshold analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8baa43",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentages = X_train.isnull().mean() * 100\n",
    "# Identify columns with more than 60% missing values \n",
    "columns_to_drop = missing_percentages[missing_percentages > 45].index\n",
    "# Drop those columns from the training set\n",
    "X_train = X_train.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703d6ec2",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c7a6c",
   "metadata": {},
   "source": [
    "## Volatilatity comparision for imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate missing percentage per column \n",
    "missing_percentage = (X_train.isnull().sum() / len(data)) * 100\n",
    "# Select columns with missing values between 40% to 65% \n",
    "cols_to_plot = missing_percentage[(missing_percentage >= 40) & \n",
    "(missing_percentage <= 65)].index\n",
    "# Calculate volatility metrics before imputation for selected columns \n",
    "volatility_before = data[cols_to_plot].std() # Using standard deviation \n",
    "as an example\n",
    "print(\"\\nVolatility Metrics Before Imputation:\") \n",
    "print(volatility_before)\n",
    "# Imputation methods\n",
    "imputation_methods = ['mean', 'median', 'knn', 'mice'] \n",
    "imputation_results = {}\n",
    "# Apply different imputation methods to selected columns only \n",
    "for method in imputation_methods:\n",
    "if method == 'mean':\n",
    "# Mean imputation\n",
    "imputer = SimpleImputer(strategy='mean') \n",
    "data_imputed =\n",
    "pd.DataFrame(imputer.fit_transform(X_train[cols_to_plot]), \n",
    "columns=cols_to_plot)\n",
    "elif method == 'median':\n",
    "# Median imputation\n",
    "imputer = SimpleImputer(strategy='median') \n",
    "data_imputed =\n",
    "pd.DataFrame(imputer.fit_transform(X_train[cols_to_plot]), \n",
    "columns=cols_to_plot)\n",
    "elif method == 'knn':\n",
    "# KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=3) \n",
    "data_imputed =\n",
    "pd.DataFrame(imputer.fit_transform(X_train[cols_to_plot]), \n",
    "columns=cols_to_plot)\n",
    "elif method == 'mice':\n",
    "# MICE (IterativeImputer) imputation \n",
    "imputer = IterativeImputer()\n",
    "data_imputed = \n",
    "pd.DataFrame(imputer.fit_transform(X_train[cols_to_plot]), \n",
    "columns=cols_to_plot)\n",
    "# Calculate volatility metrics after imputation for selected columns \n",
    "volatility_after = data_imputed.std() # Using standard deviation as an \n",
    "example\n",
    "imputation_results[method] = volatility_after\n",
    "# Plot before and after volatility comparison for selected columns only \n",
    "plt.figure(figsize=(10, 6))\n",
    "x = range(len(cols_to_plot))\n",
    "plt.bar(x, volatility_before, width=0.4, alpha=0.6, color='b', \n",
    "label='Before Imputation')\n",
    "plt.bar(x, volatility_after, width=0.4, alpha=0.6, color='r', label='After \n",
    "Imputation')\n",
    "# Add labels to the bars \n",
    "for i in x:\n",
    "plt.text(i, volatility_before[i], f'{volatility_before[i]:.2f}', ha='center', \n",
    "va='bottom', color='blue')\n",
    "plt.text(i, volatility_after[i], f'{volatility_after[i]:.2f}', ha='center', \n",
    "va='bottom', color='red')\n",
    "plt.title(f'Volatility Comparison - {method.capitalize()} Imputation') \n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Standard Deviation') \n",
    "plt.xticks(x, cols_to_plot, rotation=45) \n",
    "plt.legend()\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "# Print volatility metrics after each imputation method for selected columns only\n",
    "print(\"\\nVolatility Metrics After Imputation:\")\n",
    "for method, volatility_after in imputation_results.items(): \n",
    "print(f\"\\nMethod: {method.capitalize()}\") \n",
    "print(volatility_after)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6b372",
   "metadata": {},
   "source": [
    "## KNN Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd1cad0",
   "metadata": {},
   "source": [
    "#### kNN shows lower volatility changes after imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c248a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer \n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "# Fit the imputer to your data and transform it \n",
    "data_imputed = imputer.fit_transform(X_train)\n",
    "# Convert the imputed data back to a DataFrame\n",
    "X_train = pd.DataFrame(data_imputed, columns=X_train.columns) \n",
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "# Fit the imputer to your data and transform it \n",
    "data_imputed = imputer.fit_transform(X_test)\n",
    "# Convert the imputed data back to a DataFrame\n",
    "X_test = pd.DataFrame(data_imputed, columns=X_test.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dfa49e",
   "metadata": {},
   "source": [
    "# Feature selection/ reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046a684a",
   "metadata": {},
   "source": [
    "#### after rough feature reduction also we have almost more than 400 features which will not contribute to model building process, so select imporatnt features and build model using feature selection or reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16e9a69",
   "metadata": {},
   "source": [
    "## scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a27834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data using Min-Max scaling \n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Fit PCA\n",
    "pca = PCA() \n",
    "pca.fit(X_train)\n",
    "# Plot scree plot \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), \n",
    "pca.explained_variance_ratio_, marker='o', linestyle='--') \n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Number of Components') \n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xticks(range(1, len(pca.explained_variance_ratio_) + 1)) \n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ded8432",
   "metadata": {},
   "source": [
    "## KMO test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4f1c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "corr_matrix = np.corrcoef(X_train, rowvar=False) \n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(corr_matrix)\n",
    "# Calculate KMO statistic \n",
    "try:\n",
    "kmo_all, kmo_model = calculate_kmo(X_train) \n",
    "print(f\"\\nKMO statistic: {kmo_model}\")\n",
    "except ValueError as ve:\n",
    "print(f\"Error occurred while calculating KMO: {ve}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c78b1e3",
   "metadata": {},
   "source": [
    "## relationship of features with target variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad4aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_with_target = X_train.corrwith(y_train)\n",
    "# Sort correlation values from largest to smallest \n",
    "correlation_sorted = \n",
    "correlation_with_target.sort_values(ascending=False)\n",
    "# Print the sorted correlation Series \n",
    "print(correlation_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccefba6a",
   "metadata": {},
   "source": [
    "## BORUTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea20015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(\"C:/Users/DhruviJayPatel/AppData/Local/Programs/Python/Python312/Lib/site-packages\")\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Assuming X_train and y_train are already defined \n",
    "# X_train: DataFrame with your features\n",
    "# y_train: Series or array with your target variable\n",
    "# Initialize RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Initialize Boruta\n",
    "boruta_selector = BorutaPy(rf, n_estimators='auto', random_state=42)\n",
    "# Fit the Boruta model \n",
    "boruta_selector.fit(X_train.values, y_train)\n",
    "# Get the boolean mask of selected features \n",
    "selected_features = boruta_selector.support_\n",
    "# Get the column names of selected features \n",
    "selected_features_columns = X_train.columns[selected_features]\n",
    "# Print the selected features \n",
    "print(\"Selected Features:\") \n",
    "print(selected_features_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14838fe5",
   "metadata": {},
   "source": [
    "## BORUTA feature ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47241b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Example: Initialize Boruta and fit it\n",
    "forest = RandomForestClassifier(n_estimators=100, \n",
    "random_state=42)\n",
    "boruta_selector = BorutaPy(forest, n_estimators='auto', \n",
    "random_state=42)\n",
    "boruta_selector.fit(X_train.values, y_train.values.ravel())\n",
    "# Get feature names and Boruta rankings \n",
    "feature_names = data.columns \n",
    "boruta_rankings = boruta_selector.ranking_\n",
    "# Create a list of tuples (feature, ranking) and sort by ranking \n",
    "features_with_ranking = list(zip(feature_names, boruta_rankings)) \n",
    "features_with_ranking_sorted = sorted(features_with_ranking, \n",
    "key=lambda x: x[1])\n",
    "# Extract sorted feature names and rankings\n",
    "sorted_features = [feat for feat, rank in features_with_ranking_sorted] \n",
    "sorted_rankings = [rank for feat, rank in\n",
    "features_with_ranking_sorted]\n",
    "# Print sorted feature rankings \n",
    "print(\"Sorted Boruta feature rankings:\")\n",
    "for feat, rank in features_with_ranking_sorted: \n",
    "print(f\"{feat}: {rank}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f800b1d",
   "metadata": {},
   "source": [
    "## Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0e790d",
   "metadata": {},
   "source": [
    "#### As we are dealing with higly imbalanced dataset, it is hard for model to detect fail cases, using balancing techniques, generate new datapoints and make less biased result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a3c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix \n",
    "from imblearn.over_sampling import SMOTE, ADASYN,RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler \n",
    "from collections import Counter\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are already defined and preprocessed\n",
    "# 1. Remove features with more than 65% missing values from the training data\n",
    "\n",
    "threshold = 0.65\n",
    "missing_ratio = X_train.isnull().mean()\n",
    "features_to_drop = missing_ratio[missing_ratio > threshold].index \n",
    "X_train.drop(features_to_drop, axis=1, inplace=True) \n",
    "X_test.drop(features_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# 2. Replace outliers using a 3-sigma boundary \n",
    "\n",
    "def replace_outliers(df):\n",
    "for col in df.select_dtypes(include=[np.number]).columns: \n",
    "mean = df[col].mean()\n",
    "std = df[col].std()\n",
    "upper_bound = mean + 3 * std \n",
    "lower_bound = mean - 3 * std\n",
    "df[col] = np.clip(df[col], lower_bound, upper_bound) \n",
    "return df\n",
    "\n",
    "X_train = replace_outliers(X_train) \n",
    "X_test = replace_outliers(X_test)\n",
    "# 3. Perform kNN imputation \n",
    "imputer = KNNImputer()\n",
    "X_train_imputed = imputer.fit_transform(X_train) \n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "# Convert back to DataFrame\n",
    "X_train = pd.DataFrame(X_train_imputed, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_imputed, columns=X_test.columns)\n",
    "\n",
    "# Function to create scatter plot\n",
    "def create_scatter_plot(X, y, title, ax):\n",
    "X_sample = X.iloc[:, :2] # selecting first two features for visualization \n",
    "sns.scatterplot(x=X_sample.iloc[:, 0], y=X_sample.iloc[:, 1], \n",
    "hue=y.map({1: 'blue', -1: 'red'}), ax=ax, palette=['blue', 'red']) \n",
    "ax.set_title(title)\n",
    "# Count the number of pass and fail cases \n",
    "counts = Counter(y)\n",
    "pass_count = counts.get(-1, 0) \n",
    "fail_count = counts.get(1, 0)\n",
    "# Add text annotations to the plot\n",
    "textstr = f'Majority (Pass): {pass_count}\\nMinority (Fail): {fail_count}' \n",
    "props = dict(boxstyle='round', facecolor='wheat', alpha=0.5) \n",
    "ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=12, verticalalignment='top', bbox=props)\n",
    "\n",
    "# Remove legend \n",
    "ax.get_legend().remove()\n",
    "# Sampling methods \n",
    "\n",
    "sampling_methods = { \n",
    "    \n",
    "'Original': (X_train, y_train), \n",
    "'Undersampling': RandomUnderSampler(random_state=42).fit_resample(X_train, \n",
    "y_train),\n",
    "'Oversampling': RandomOverSampler(random_state=42).fit_resample(X_train, \n",
    "y_train),\n",
    "'SMOTE': SMOTE(random_state=42).fit_resample(X_train, y_train), \n",
    "'ADASYN': ADASYN(random_state=42).fit_resample(X_train, \n",
    "y_train),\n",
    "'ROSE': RandomOverSampler(random_state=42).fit_resample(X_train, \n",
    "y_train)\n",
    "    \n",
    "}\n",
    "# Plotting scatter plots\n",
    "fig, axes = plt.subplots(3, 2, figsize=(18, 18)) \n",
    "axes = axes.ravel()\n",
    "for ax, (method, (X_resampled, y_resampled)) in zip(axes, \n",
    "sampling_methods.items()): \n",
    "create_scatter_plot(X_resampled, y_resampled, method, ax)\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "# Training and evaluation\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "for method, (X_resampled, y_resampled) in \n",
    "sampling_methods.items():\n",
    "# Reversing class labels for training\n",
    "y_resampled_reversed = np.where(y_resampled == 1, -1, 1) \n",
    "knn.fit(X_resampled, y_resampled_reversed)\n",
    "y_pred = knn.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred) \n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Method: {method}\") \n",
    "print(f\"Accuracy: {acc}\") \n",
    "print(\"Confusion Matrix:\") \n",
    "print(conf_matrix) \n",
    "print(\"\\n\")\n",
    "# Print the number of pass and fail cases \n",
    "counts_resampled = Counter(y_resampled) \n",
    "pass_count_resampled = counts_resampled.get(-1, 0) \n",
    "fail_count_resampled = counts_resampled.get(1, 0)\n",
    "print(f\"Resampled Pass (Majority): {pass_count_resampled}, \n",
    "Resampled Fail (Minority): {fail_count_resampled}\") \n",
    "print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4d08d",
   "metadata": {},
   "source": [
    "# 3. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9305197",
   "metadata": {},
   "source": [
    "## model 1 - Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f491b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from boruta import BorutaPy\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Assuming 'X_train', 'y_train', 'X_test', 'y_test' are your training and test sets\n",
    "# Step 1: Load and preprocess your data\n",
    "# Example: Remove features with >65% missing values\n",
    "threshold = 0.65\n",
    "missing_counts = X_train.isnull().sum()\n",
    "cols_to_remove = missing_counts[missing_counts / len(X_train) > threshold].index\n",
    "X_train = X_train.drop(cols_to_remove, axis=1)\n",
    "X_test = X_test.drop(cols_to_remove, axis=1)\n",
    "# Step 2: Handle outliers using Z-score and replace with NaN\n",
    "def handle_outliers_zscore(df, cols=None, threshold=3):\n",
    "if cols is None:\n",
    "cols = df.select_dtypes(include=[np.number]).columns\n",
    "for col in cols:\n",
    "z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())\n",
    "df[col] = np.where(z_scores > threshold, np.nan, df[col])\n",
    "return df\n",
    "# Apply outlier handling using Z-score to both X_train and X_test\n",
    "X_train = handle_outliers_zscore(X_train)\n",
    "X_test = handle_outliers_zscore(X_test)\n",
    "# Step 3: KNN imputation to replace outliers and other missing values\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "X_train_imputed = pd.DataFrame(knn_imputer.fit_transform(X_train), \n",
    "columns=X_train.columns)\n",
    "X_test_imputed = pd.DataFrame(knn_imputer.transform(X_test), columns=X_test.columns)\n",
    "# Step 4: Feature Selection using Boruta\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "boruta_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    "boruta_selector.fit(X_train_imputed.values, y_train.values)\n",
    "selected_features = X_train_imputed.columns[boruta_selector.support_]\n",
    "X_train_selected = X_train_imputed[selected_features]\n",
    "X_test_selected = X_test_imputed[selected_features]\n",
    "# Step 5: Handling imbalanced dataset using SMOTE\n",
    "smote = SMOTE(random_state=1)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_selected, y_train)\n",
    "# Step 6: Train the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=1)\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "# Step 7: Model evaluation\n",
    "# Predict on training set\n",
    "y_train_pred = rf_model.predict(X_train_balanced)\n",
    "train_accuracy = accuracy_score(y_train_balanced, y_train_pred)\n",
    "train_error = 1 - train_accuracy\n",
    "train_confusion_matrix = confusion_matrix(y_train_balanced, y_train_pred)\n",
    "# Predict on test set\n",
    "y_test_pred = rf_model.predict(X_test_selected)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_error = 1 - test_accuracy\n",
    "test_confusion_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"Train Confusion Matrix:\")\n",
    "print(train_confusion_matrix)\n",
    "print(\"Train Accuracy:\", train_accuracy)\n",
    "print(\"Train Error:\", train_error)\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(test_confusion_matrix)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test Error:\", test_error)\n",
    "# Print model accuracy\n",
    "print(\"\\nModel Accuracy on Test Set:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa67961",
   "metadata": {},
   "source": [
    "## Customised model with boruta features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e290cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train, y_train, X_test, y_test are already defined\n",
    "# 1. Remove features with more than 65% missing values \n",
    "threshold = 0.65\n",
    "missing_ratio = X_train.isnull().mean()\n",
    "features_to_drop = missing_ratio[missing_ratio > threshold].index \n",
    "X_train.drop(features_to_drop, axis=1, inplace=True) \n",
    "X_test.drop(features_to_drop, axis=1, inplace=True)\n",
    "# 2. Replace outliers using a 3-sigma boundary \n",
    "def replace_outliers(df):\n",
    "for col in df.select_dtypes(include=[np.number]).columns: \n",
    "mean = df[col].mean()\n",
    "std = df[col].std()\n",
    "upper_bound = mean + 3 * std \n",
    "lower_bound = mean - 3 * std\n",
    "df[col] = np.clip(df[col], lower_bound, upper_bound) \n",
    "return df\n",
    "X_train = replace_outliers(X_train) \n",
    "X_test = replace_outliers(X_test)\n",
    "# 3. Perform kNN imputation \n",
    "imputer = KNNImputer()\n",
    "X_train_imputed = imputer.fit_transform(X_train) \n",
    "X_test_imputed = imputer.transform(X_test)\n",
    "# Convert back to DataFrame\n",
    "X_train = pd.DataFrame(X_train_imputed, columns=X_train.columns) \n",
    "X_test = pd.DataFrame(X_test_imputed, columns=X_test.columns\n",
    " # 4. Select specified features for model building\n",
    "selected_features = ['feature60', 'feature65', 'feature66', 'feature342', 'feature351', 'feature478', \n",
    "'feature540', 'feature563']\n",
    "X_train = X_train[selected_features] \n",
    "X_test = X_test[selected_features]\n",
    "# Apply SMOTE for balancing the training data \n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "# Train Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42) \n",
    "rf.fit(X_train_smote, y_train_smote)\n",
    "# Make predictions on the test set \n",
    "y_pred = rf.predict(X_test)\n",
    "# Evaluate the model\n",
    "acc = accuracy_score(y_test, y_pred) \n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc}\") \n",
    "print(\"Confusion Matrix:\") \n",
    "print(conf_matrix) \n",
    "print(\"Classification Report:\") \n",
    "print(class_report)\n",
    "# Calculate and print the loss cost\n",
    "def calculate_loss_cost(conf_matrix, cost_fp, cost_fn): \n",
    "# Confusion matrix format:\n",
    "# [[TN, FP],\n",
    "# [FN, TP]]\n",
    "tn, fp, fn, tp = conf_matrix.ravel() \n",
    "loss_cost = (fp * cost_fp) + (fn * cost_fn) \n",
    "return loss_cost\n",
    "cost_fp = 1000 # Example cost for False Positive \n",
    "cost_fn = 5000 # Example cost for False Negative\n",
    "loss_cost = calculate_loss_cost(conf_matrix, cost_fp, cost_fn) \n",
    "print(f\"Loss Cost: {loss_cost}\")\n",
    "# Plot feature importance\n",
    "feature_importances = rf.feature_importances_ \n",
    "indices = np.argsort(feature_importances)[::-1]\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=feature_importances[indices], y=np.array(selected_features)[indices]) \n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance') \n",
    "plt.ylabel('Features') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76e83b6",
   "metadata": {},
   "source": [
    "# PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9494881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Remove features with missing values above the threshold \n",
    "thresholds = [45, 50, 55, 60, 65]\n",
    "def remove_features_with_missing_values(X, threshold):\n",
    "return X.loc[:, X.isnull().mean() * 100 < threshold]\n",
    "# Step 3: Handle outliers - replace with 3 standard deviation boundaries and put NA for each outlier value\n",
    "def handle_outliers(df):\n",
    "for col in df.select_dtypes(include=[np.number]).columns: \n",
    "upper_bound = df[col].mean() + 3 * df[col].std() \n",
    "lower_bound = df[col].mean() - 3 * df[col].std()\n",
    "df[col] = np.where((df[col] > upper_bound) | (df[col] < lower_bound), np.nan, df[col]) \n",
    "return df\n",
    "# Step 4: Missing value imputation\n",
    "def impute_missing_values(df, method): \n",
    "if method == 'KNN':\n",
    "imputer = KNNImputer() \n",
    "elif method == 'MICE':\n",
    "imputer = IterativeImputer() \n",
    "elif method == 'Mean':\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "return pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "# Step 5: Feature selection/reduction \n",
    "# Boruta\n",
    "def boruta_feature_selection(X, y):\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5) \n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=42) \n",
    "feat_selector.fit(X.values, y.values)\n",
    "selected_features = X.columns[feat_selector.support_].tolist() \n",
    "return selected_features, 'Boruta'\n",
    "# PCA\n",
    "def pca_reduction(X, n_components):\n",
    "pca = PCA(n_components=n_components) \n",
    "return pca.fit_transform(X), 'PCA'\n",
    "# Step 6: Data sampling for imbalanced dataset \n",
    "def handle_imbalance(X, y, method):\n",
    "if method == 'SMOTE':\n",
    "smote = SMOTE(random_state=42) \n",
    "X_res, y_res = smote.fit_resample(X, y) \n",
    "elif method == 'RandomOverSampler':\n",
    "ros = RandomOverSampler(random_state=42) \n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "return X_res, y_res\n",
    "# Function to calculate Loss Cost based on Confusion Matrix \n",
    "def calculate_loss_cost(conf_matrix, cost_fp, cost_fn): \n",
    "total_fp = conf_matrix[0, 1]\n",
    "total_fn = conf_matrix[1, 0]\n",
    "loss_cost = total_fp * cost_fp + total_fn * cost_fn \n",
    "return loss_cost\n",
    "# List to store model performance \n",
    "results = []\n",
    "boruta_selected_features = [] # List to collect Boruta selected features\n",
    "# Iterate through thresholds \n",
    "for threshold in thresholds:\n",
    "X_train_thresh = remove_features_with_missing_values(X_train, threshold) \n",
    "X_test_thresh = X_test[X_train_thresh.columns]\n",
    "# Handle outliers\n",
    "X_train_outliers = handle_outliers(X_train_thresh) \n",
    "X_test_outliers = handle_outliers(X_test_thresh)\n",
    "# Iterate through feature selection/reduction methods\n",
    "for feature_method_choice in ['Boruta', 'PCA']:\n",
    "# Iterate through imputation methods\n",
    "for impute_method in ['KNN', 'MICE', 'Mean']:\n",
    "X_train_imputed = impute_missing_values(X_train_outliers, impute_method) \n",
    "X_test_imputed = impute_missing_values(X_test_outliers, impute_method)\n",
    "# Perform feature selection/reduction based on choice \n",
    "if feature_method_choice == 'Boruta':\n",
    "selected_features, feature_method = boruta_feature_selection(X_train_imputed, y_train) \n",
    "X_train_selected = X_train_imputed[selected_features]\n",
    "X_test_selected = X_test_imputed[selected_features] \n",
    "elif feature_method_choice == 'PCA':\n",
    "X_train_selected, feature_method = pca_reduction(X_train_imputed, n_components=30) \n",
    "X_test_selected = pca_reduction(X_test_imputed, n_components=30)[0]\n",
    "# Iterate through imbalance methods\n",
    "for imbalance_method in ['SMOTE', 'RandomOverSampler']:\n",
    "X_train_res, y_train_res = handle_imbalance(X_train_selected, y_train, imbalance_method)\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42) \n",
    "rf_model.fit(X_train_res, y_train_res)\n",
    "# Predictions\n",
    "y_pred = rf_model.predict(X_test_selected)\n",
    "# Evaluation metrics\n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "conf_matrix = confusion_matrix(y_test, y_pred) \n",
    "precision = precision_score(y_test, y_pred) \n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred)\n",
    "# Calculate Loss Cost\n",
    "cost_fp = 1000 # Example cost for False Positive \n",
    "cost_fn = 5000 # Example cost for False Negative\n",
    "loss_cost = calculate_loss_cost(conf_matrix, cost_fp, cost_fn)\n",
    "# Prepare results \n",
    "result = {\n",
    "'threshold': threshold, \n",
    "'impute_method': impute_method,\n",
    "'imbalance_method': imbalance_method, \n",
    "'accuracy': accuracy,\n",
    "'confusion_matrix': conf_matrix, \n",
    "'precision': precision,\n",
    "'recall': recall, \n",
    "'f1_score': f1, \n",
    "'auc': auc,\n",
    "'loss_cost': loss_cost, \n",
    "'features_used': feature_method,\n",
    "'selected_features': selected_features if feature_method == 'Boruta' else None\n",
    "}\n",
    "results.append(result)\n",
    "# Collect Boruta selected features \n",
    "if feature_method == 'Boruta':\n",
    "boruta_selected_features.append(selected_features)\n",
    "# Convert results to DataFrame \n",
    "results_df = pd.DataFrame(results)\n",
    "# Display top 10 models by loss cost\n",
    "top_10_models_loss_cost = results_df.sort_values(by='loss_cost', ascending=True).head(10) \n",
    "print(\"\\nTop 10 Models by Loss Cost:\")\n",
    "print(top_10_models_loss_cost[['threshold', 'impute_method', 'imbalance_method', 'accuracy', \n",
    "'confusion_matrix',\n",
    "'precision', 'recall', 'f1_score', 'auc', 'loss_cost', 'features_used', 'selected_features']])\n",
    "# Display top 10 models by minimizing False Positive and False Negative errors\n",
    "top_10_models_fp_fn = results_df.sort_values(by=['confusion_matrix'], key=lambda x: x.apply(lambda \n",
    "y: (y[0][1], y[1][0])), ascending=True).head(10)\n",
    "print(\"\\nTop 10 Models by False Positive and False Negative Errors:\") \n",
    "print(top_10_models_fp_fn[['threshold', 'impute_method', 'imbalance_method', 'accuracy', \n",
    "'confusion_matrix',\n",
    "'precision', 'recall', 'f1_score', 'auc', 'loss_cost', 'features_used', 'selected_features']])\n",
    "# Print Boruta selected features if present in top models by loss cost \n",
    "if boruta_selected_features:\n",
    "print(\"\\nBoruta Selected Features in Top Models:\")\n",
    "for idx, features in enumerate(boruta_selected_features, start=1): \n",
    "print(f\"Iteration {idx}: {features}\")\n",
    "else:\n",
    "print(\"\\nNo Boruta selected features in Top Models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa520b4",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8537757",
   "metadata": {},
   "source": [
    "#### Features with higher ranges are more likely to be chosen by the model.Scaling ensures uniformity, improves performance of algorithms, and reduces biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy import stats\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame with 'feature17'\n",
    "feature17 = merged_df['feature17'].values.reshape(-1, 1)\n",
    "\n",
    "# Apply different transformations\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler_minmax = MinMaxScaler()\n",
    "feature17_minmax = scaler_minmax.fit_transform(feature17)\n",
    "\n",
    "# Log Transformation\n",
    "feature17_log = np.log(feature17 + 1)  # Adding 1 to handle zeros\n",
    "\n",
    "# Linear Transformation\n",
    "feature17_linear = 0.5 * feature17  # Scaling by a factor of 0.5\n",
    "\n",
    "# Box-Cox Transformation\n",
    "feature17_boxcox, _ = stats.boxcox(feature17.flatten() + 1)  # Adding 1 to handle zeros\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Min-Max Scaling\n",
    "axes[0, 0].hist(feature17_minmax.flatten(), bins=30, color='green', alpha=0.7)\n",
    "axes[0, 0].set_title('Min-Max Scaling (Feature 17)')\n",
    "\n",
    "# Log Transformation\n",
    "axes[0, 1].hist(feature17_log.flatten(), bins=30, color='purple', alpha=0.7)\n",
    "axes[0, 1].set_title('Log Transformation (Feature 17)')\n",
    "\n",
    "# Linear Transformation\n",
    "axes[1, 0].hist(feature17_linear.flatten(), bins=30, color='orange', alpha=0.7)\n",
    "axes[1, 0].set_title('Linear Transformation (Feature 17)')\n",
    "\n",
    "# Box-Cox Transformation\n",
    "axes[1, 1].hist(feature17_boxcox, bins=30, color='red', alpha=0.7)\n",
    "axes[1, 1].set_title('Box-Cox Transformation (Feature 17)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b542d928",
   "metadata": {},
   "source": [
    "# Optimal parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e8f3af",
   "metadata": {},
   "source": [
    "#### to find out the best possible flow for every model in which the evaluation matrics like precision, recall, f1 score and loss cost will be economically suitable for the industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f8498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'NaiveBayes': GaussianNB(),\n",
    "    'SVM': SVC(),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Define feature selection and balancing techniques\n",
    "feature_methods = ['PCA', 'Boruta']\n",
    "balance_methods = ['SMOTE', 'ADASYN', 'ROSE']\n",
    "\n",
    "# Function to apply PCA\n",
    "def apply_pca(X_train, X_test, n_components=10):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    return X_train_pca, X_test_pca\n",
    "\n",
    "# Function to apply Boruta\n",
    "def apply_boruta(X_train, y_train, X_test):\n",
    "    rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "    boruta = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=1)\n",
    "    boruta.fit(X_train.values, y_train)\n",
    "    X_train_boruta = boruta.transform(X_train.values)\n",
    "    X_test_boruta = boruta.transform(X_test.values)\n",
    "    return X_train_boruta, X_test_boruta\n",
    "\n",
    "# Balancing functions\n",
    "def balance_data(X_train, y_train, method):\n",
    "    if method == 'SMOTE':\n",
    "        smote = SMOTE(random_state=42)\n",
    "        X_res, y_res = smote.fit_resample(X_train, y_train)\n",
    "    elif method == 'ADASYN':\n",
    "        adasyn = ADASYN(random_state=42)\n",
    "        X_res, y_res = adasyn.fit_resample(X_train, y_train)\n",
    "    elif method == 'ROSE':\n",
    "        rose = SMOTETomek(random_state=42)\n",
    "        X_res, y_res = rose.fit_resample(X_train, y_train)\n",
    "    return X_res, y_res\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    loss = fp * 1000 + fn * 5000\n",
    "    return f1, precision, recall, loss\n",
    "\n",
    "# Collecting results\n",
    "results = []\n",
    "for feature_method in feature_methods:\n",
    "    if feature_method == 'PCA':\n",
    "        X_train_fs, X_test_fs = apply_pca(X_train_imputed, X_test_imputed)\n",
    "    elif feature_method == 'Boruta':\n",
    "        X_train_fs, X_test_fs = apply_boruta(X_train_imputed, y_train, X_test_imputed)\n",
    "\n",
    "    for balance_method in balance_methods:\n",
    "        X_train_bal, y_train_bal = balance_data(X_train_fs, y_train, balance_method)\n",
    "        \n",
    "        # Apply scaling where necessary\n",
    "        scaler = StandardScaler()\n",
    "        X_train_bal_scaled = scaler.fit_transform(X_train_bal)\n",
    "        X_test_fs_scaled = scaler.transform(X_test_fs)\n",
    "\n",
    "        for model_name, model in models.items():\n",
    "            if model_name in ['SVM', 'KNN', 'LogisticRegression']:\n",
    "                f1, precision, recall, loss = evaluate_model(model, X_train_bal_scaled, y_train_bal, X_test_fs_scaled, y_test)\n",
    "            else:\n",
    "                f1, precision, recall, loss = evaluate_model(model, X_train_bal, y_train_bal, X_test_fs, y_test)\n",
    "            results.append([f'{feature_method}+{balance_method}', model_name, f1, precision, recall, loss])\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results, columns=['Method', 'Model', 'F1 Score', 'Precision', 'Recall', 'Loss'])\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot F1 Score\n",
    "x = np.arange(len(results_df['Method'].unique()))\n",
    "width = 0.15\n",
    "\n",
    "for i, model_name in enumerate(models.keys()):\n",
    "    subset = results_df[results_df['Model'] == model_name]\n",
    "    axs[0, 0].bar(x + i*width, subset['F1 Score'], width, label=model_name)\n",
    "axs[0, 0].set_title('F1 Score')\n",
    "axs[0, 0].set_xticks(x + width*(len(models)/2))\n",
    "axs[0, 0].set_xticklabels(results_df['Method'].unique(), rotation=90)\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Plot Precision\n",
    "for i, model_name in enumerate(models.keys()):\n",
    "    subset = results_df[results_df['Model'] == model_name]\n",
    "    axs[0, 1].bar(x + i*width, subset['Precision'], width, label=model_name)\n",
    "axs[0, 1].set_title('Precision')\n",
    "axs[0, 1].set_xticks(x + width*(len(models)/2))\n",
    "axs[0, 1].set_xticklabels(results_df['Method'].unique(), rotation=90)\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Plot Recall\n",
    "for i, model_name in enumerate(models.keys()):\n",
    "    subset = results_df[results_df['Model'] == model_name]\n",
    "    axs[1, 0].bar(x + i*width, subset['Recall'], width, label=model_name)\n",
    "axs[1, 0].set_title('Recall')\n",
    "axs[1, 0].set_xticks(x + width*(len(models)/2))\n",
    "axs[1, 0].set_xticklabels(results_df['Method'].unique(), rotation=90)\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Plot Loss\n",
    "for i, model_name in enumerate(models.keys()):\n",
    "    subset = results_df[results_df['Model'] == model_name]\n",
    "    axs[1, 1].bar(x + i*width, subset['Loss'], width, label=model_name)\n",
    "axs[1, 1].set_title('Loss')\n",
    "axs[1, 1].set_xticks(x + width*(len(models)/2))\n",
    "axs[1, 1].set_xticklabels(results_df['Method'].unique(), rotation=90)\n",
    "axs[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3dd156",
   "metadata": {},
   "source": [
    "## hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b42042",
   "metadata": {},
   "source": [
    "#### Discovering the most suitable combination of hyperparameters for a machine learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0ac72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from boruta import BorutaPy\n",
    "\n",
    "\n",
    "# Selecting specific features identified by Boruta\n",
    "selected_features = ['feature60', 'feature65', 'feature66', 'feature342', 'feature351', 'feature478', 'feature540', 'feature563']\n",
    "\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# SMOTE Data Balancing\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = sm.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# Scaling the Features for SVM only\n",
    "scaler = StandardScaler()\n",
    "X_train_res_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Function to evaluate model and return evaluation metrics\n",
    "def evaluate_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    return accuracy, precision, recall, f1, conf_matrix, y_pred_prob\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    SVC(probability=True),\n",
    "    GaussianNB(),\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "]\n",
    "\n",
    "# Define colors for each model consistently\n",
    "model_colors = {\n",
    "    'RandomForestClassifier': 'green',\n",
    "    'GaussianNB': 'lightgreen',\n",
    "    'SVC': 'red'\n",
    "}\n",
    "\n",
    "# Lists to store results before and after tuning\n",
    "models_results_before = []\n",
    "models_results_after = []\n",
    "roc_curves_before = []\n",
    "roc_curves_after = []\n",
    "\n",
    "# Evaluate each model before tuning\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    color = model_colors[model_name]\n",
    "\n",
    "    if isinstance(model, SVC):\n",
    "        accuracy, precision, recall, f1, conf_matrix, y_pred_prob = evaluate_model(model, X_train_res_scaled, y_train_res, X_test_scaled, y_test)\n",
    "    else:\n",
    "        accuracy, precision, recall, f1, conf_matrix, y_pred_prob = evaluate_model(model, X_train_res, y_train_res, X_test_selected, y_test)\n",
    "\n",
    "    models_results_before.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Color': color\n",
    "    })\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_curves_before.append((fpr, tpr, roc_auc, model_name, color))\n",
    "\n",
    "# Define parameter grids for tuning\n",
    "param_grids = {\n",
    "    'SVC': {'C': [0.1, 1, 10], 'gamma': ['scale', 'auto'], 'kernel': ['linear', 'rbf']},\n",
    "    'GaussianNB': {'var_smoothing': [1e-09, 1e-08, 1e-07]},\n",
    "    'RandomForestClassifier': {'max_depth': [None, 5, 10]}\n",
    "}\n",
    "\n",
    "# Define k-fold Cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate each model after tuning with k-fold\n",
    "for model in models:\n",
    "    model_name = model.__class__.__name__\n",
    "    param_grid = param_grids[model_name]\n",
    "    color = model_colors[model_name]\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, scoring='roc_auc', cv=kf, n_jobs=-1)\n",
    "    if isinstance(model, SVC):\n",
    "        grid_search.fit(X_train_res_scaled, y_train_res)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        accuracy, precision, recall, f1, conf_matrix, y_pred_prob = evaluate_model(best_model, X_train_res_scaled, y_train_res, X_test_scaled, y_test)\n",
    "    else:\n",
    "        grid_search.fit(X_train_res, y_train_res)\n",
    "        best_model = grid_search.best_estimator_\n",
    "        accuracy, precision, recall, f1, conf_matrix, y_pred_prob = evaluate_model(best_model, X_train_res, y_train_res, X_test_selected, y_test)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    best_score = grid_search.best_score_\n",
    "\n",
    "    print(f\"\"Best Parameters for {model_name}: {best_params}\"\")\n",
    "    print(f\"\"Best ROC AUC Score: {best_score}\"\")\n",
    "\n",
    "    models_results_after.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'Confusion Matrix': conf_matrix,\n",
    "        'Color': color\n",
    "    })\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    roc_curves_after.append((fpr, tpr, roc_auc, model_name, color))\n",
    "\n",
    "# Plotting comparison graphs\n",
    "metrics = ['Precision', 'Recall', 'F1 Score', 'Accuracy']\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    index = np.arange(len(models_results_before))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    before_values = [result[metric] for result in models_results_before]\n",
    "    after_values = [result[metric] for result in models_results_after]\n",
    "    colors = [result['Color'] for result in models_results_before]\n",
    "\n",
    "    bars1 = ax.bar(index, before_values, bar_width, color=colors, alpha=0.6)\n",
    "    bars2 = ax.bar(index + bar_width, after_values, bar_width, color=colors, alpha=1.0)\n",
    "\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels([result['Model'] for result in models_results_before])\n",
    "\n",
    "    for bar in bars1:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, yval / 2, f'{yval:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "    for bar in bars2:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, yval / 2, f'{yval:.2f}', ha='center', va='center', color='white')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# Add single legend for all subplots\n",
    "handles = [plt.Rectangle((0,0),1,1, color='gray', alpha=0.6, label='Before Tuning'),\n",
    "           plt.Rectangle((0,0),1,1, color='gray', alpha=1.0, label='After Tuning')]\n",
    "fig.legend(handles=handles, loc='upper center', bbox_to_anchor=(0.5, 1.05), ncol=2)\n",
    "plt.show()\n",
    "\n",
    "# Plotting TP, TN, FP, FN side by side with labels\n",
    "fig, axes = plt.subplots(1, 4, figsize=(18, 6))\n",
    "metrics_names = ['TN', 'FN', 'FP', 'TP']\n",
    "\n",
    "# Confusion matrix plotting function\n",
    "def plot_conf_matrix(metric_name, i, ax):\n",
    "    index = np.arange(len(models_results_before))\n",
    "    bar_width = 0.35\n",
    "\n",
    "    before_values = [result['Confusion Matrix'].ravel()[i] for result in models_results_before]\n",
    "    after_values = [result['Confusion Matrix'].ravel()[i] for result in models_results_after]\n",
    "    colors = [result['Color'] for result in models_results_before]\n",
    "\n",
    "    bars1 = ax.bar(index, before_values, bar_width, color=colors, alpha=0.6)\n",
    "    bars2 = ax.bar(index + bar_width, after_values, bar_width, color=colors, alpha=1.0)\n",
    "\n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Counts')\n",
    "    ax.set_title(f'{metric_name} Comparison')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels([result['Model'] for result in models_results_before])\n",
    "\n",
    "    for bar in bars1:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, yval / 2, f'{yval:.0f}', ha='center', va='center', color='white')\n",
    "\n",
    "    for bar in bars2:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, yval / 2, f'{yval:.0f}', ha='center', va='center', color='white')\n",
    "\n",
    "for i, metric_name in enumerate(metrics_names):\n",
    "    plot_conf_matrix(metric_name, i, axes[i])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot ROC Curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "for fpr, tpr, roc_auc, model_name, color in roc_curves_before:\n",
    "    plt.plot(fpr, tpr, lw=2, linestyle='--', color=color, label=f'{model_name} Before (AUC = {roc_auc:.2f})')\n",
    "\n",
    "for fpr, tpr, roc_auc, model_name, color in roc_curves_after:\n",
    "    plt.plot(fpr, tpr, lw=2, color=color, label=f'{model_name} After (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "\n",
    "# Create a single legend outside the plot for ROC Curves\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Function to calculate loss cost from confusion matrix\n",
    "def calculate_loss_cost(conf_matrix):\n",
    "    # Define the costs for each type of misclassification\n",
    "    cost_fp = 1000  # Cost of False Positive\n",
    "    cost_fn = 5000  # Cost of False Negative\n",
    "\n",
    "    # Extract values from confusion matrix\n",
    "    FP = conf_matrix[0, 1]\n",
    "    FN = conf_matrix[1, 0]\n",
    "\n",
    "    # Calculate total loss cost\n",
    "    total_cost = FP * cost_fp + FN * cost_fn\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "# Extracting model names and corresponding colors\n",
    "model_names = [result['Model'] for result in models_results_before]\n",
    "colors = [result['Color'] for result in models_results_before]\n",
    "\n",
    "# Loss costs before and after tuning\n",
    "loss_costs_before = [calculate_loss_cost(result['Confusion Matrix']) for result in models_results_before]\n",
    "loss_costs_after = [calculate_loss_cost(result['Confusion Matrix']) for result in models_results_after]\n",
    "\n",
    "# Setting up the figure\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(model_names))\n",
    "\n",
    "bars1 = ax.bar(index - bar_width/2, loss_costs_before, bar_width, color=colors, alpha=0.6, label='Before Tuning')\n",
    "bars2 = ax.bar(index + bar_width/2, loss_costs_after, bar_width, color=colors, alpha=1.0, label='After Tuning')\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Loss Cost')\n",
    "ax.set_title('Loss Cost Comparison Before and After Tuning')\n",
    "ax.set_xticks(index)\n",
    "ax.set_xticklabels(model_names)\n",
    "ax.legend()\n",
    "\n",
    "# Adding text labels for values on top of bars\n",
    "def autolabel(bars):\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval / 2, f'{yval:.0f}', ha='center', va='center', color='white')\n",
    "\n",
    "autolabel(bars1)\n",
    "autolabel(bars2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c20654",
   "metadata": {},
   "source": [
    "## Learining curve and Model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97501364",
   "metadata": {},
   "source": [
    "#### to check how model performed, to check it is optimum, overfit or underfit. can be identified using training score and validation score- how model has performed on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1213296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "from sklearn.model_selection import cross_val_score, learning_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cross-validation scores\n",
    "def cross_val_evaluation(model, X, y, cv=kf):\n",
    "    accuracy = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "    precision = cross_val_score(model, X, y, cv=cv, scoring='precision')\n",
    "    recall = cross_val_score(model, X, y, cv=cv, scoring='recall')\n",
    "    f1 = cross_val_score(model, X, y, cv=cv, scoring='f1')\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "# Learning curves\n",
    "def plot_learning_curve(estimator, title, X, y, cv=kf):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"\"Training examples\"\")\n",
    "    plt.ylabel(\"\"Score\"\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=-1)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"\"r\"\", label=\"\"Training score\"\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"\"g\"\", label=\"\"Cross-validation score\"\")\n",
    "\n",
    "    plt.legend(loc=\"\"best\"\")\n",
    "    plt.show()\n",
    "\n",
    "#  usage for cross-validation\n",
    "models = [\n",
    "    (SVC(probability=True), X_train_res_scaled, y_train_res),\n",
    "    (GaussianNB(), X_train_res, y_train_res),\n",
    "    (RandomForestClassifier(n_estimators=100, random_state=42), X_train_res, y_train_res)\n",
    "]\n",
    "\n",
    "for model, X, y in models:\n",
    "    acc, prec, rec, f1 = cross_val_evaluation(model, X, y)\n",
    "    print(f\"\"{model.__class__.__name__} - Accuracy: {np.mean(acc):.2f}, Precision: {np.mean(prec):.2f}, Recall: {np.mean(rec):.2f}, F1 Score: {np.mean(f1):.2f}\"\")\n",
    "\n",
    "# Example usage for learning curves\n",
    "plot_learning_curve(SVC(probability=True), \"\"Learning Curve (SVM)\"\", X_train_res_scaled, y_train_res)\n",
    "plot_learning_curve(GaussianNB(), \"\"Learning Curve (Naive Bayes)\"\", X_train_res, y_train_res)\n",
    "plot_learning_curve(RandomForestClassifier(n_estimators=100, random_state=42), \"\"Learning Curve (Random Forest)\"\", X_train_res, y_train_res)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66bf525",
   "metadata": {},
   "source": [
    "# Graphs- precision, f1, recall, lost... 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebcba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Data for three models before and after tuning\n",
    "models = ['Model 1', 'Model 2', 'Model 3']\n",
    "metrics = ['Accuracy', 'F1 Score', 'Precision', 'Recall (Sensitivity)', 'Loss Cost']\n",
    "metrics_values_before = np.array([\n",
    "    [0.8086734694, 0.8494935529, 0.9134543088, 0.8086734694, 119000],\n",
    "    [0.9107142857, 0.2553191489, 0.2857142857, 0.2307692308, 115000],\n",
    "    [0.5816326531, 0.6856787933, 0.8740304301, 0.5816326531, 228000]\n",
    "])\n",
    "metrics_values_after = np.array([\n",
    "    [0.8826530612, 0.8926764456, 0.904544958, 0.8826530612, 114000],\n",
    "    [0.9285714286, 0.9184331797, 0.9126984127, 0.9285714286, 108000],\n",
    "    [0.5816326531, 0.6856787933, 0.8740304301, 0.5816326531, 228000]\n",
    "])\n",
    "\n",
    "confusion_matrix_before = np.array([\n",
    "    [302, 64, 11, 15],\n",
    "    [274, 19, 19, 2],\n",
    "    [218, 148, 16, 10]\n",
    "])\n",
    "\n",
    "confusion_matrix_after = np.array([\n",
    "    [337, 29, 17, 9],\n",
    "    [358, 8, 20, 6],\n",
    "    [218, 148, 16, 10]\n",
    "])\n",
    "\n",
    "# Define colors for models\n",
    "colors = {\n",
    "    'Model 1': {'Before Tuning': 'lightgreen', 'After Tuning': 'green'},\n",
    "    'Model 2': {'Before Tuning': 'mediumseagreen', 'After Tuning': 'seagreen'},  # Using mediumseagreen for Model 2 before tuning\n",
    "    'Model 3': {'Before Tuning': 'lightcoral', 'After Tuning': 'red'}\n",
    "}\n",
    "\n",
    "# Function to plot side-by-side comparison of metrics\n",
    "def plot_metrics_comparison(metrics, metrics_values_before, metrics_values_after):\n",
    "    num_metrics = len(metrics)\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "    for i in range(num_metrics):\n",
    "        row = i // 3\n",
    "        col = i % 3\n",
    "        ax = axes[row, col]\n",
    "        \n",
    "        index = np.arange(len(models))\n",
    "        bar_width = 0.35\n",
    "        \n",
    "        before_vals = metrics_values_before[:, i]\n",
    "        after_vals = metrics_values_after[:, i]\n",
    "        \n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(index[j] - bar_width/2, before_vals[j], bar_width, \n",
    "                   label='Before Tuning', color=colors[model]['Before Tuning'], alpha=0.6)\n",
    "            ax.bar(index[j] + bar_width/2, after_vals[j], bar_width, \n",
    "                   label='After Tuning', color=colors[model]['After Tuning'], alpha=1.0)\n",
    "\n",
    "            if metrics[i] == 'Loss Cost':  # Adjusting format for Loss Cost metric\n",
    "                ax.text(index[j] - bar_width/2, before_vals[j] + 5000, f'{before_vals[j]:,.0f}', ha='center', va='bottom')\n",
    "                ax.text(index[j] + bar_width/2, after_vals[j] + 5000, f'{after_vals[j]:,.0f}', ha='center', va='bottom')\n",
    "            else:\n",
    "                ax.text(index[j] - bar_width/2, before_vals[j] + 0.0005, f'{before_vals[j]:.3f}', ha='center', va='bottom')\n",
    "                ax.text(index[j] + bar_width/2, after_vals[j] + 0.0005, f'{after_vals[j]:.3f}', ha='center', va='bottom')\n",
    "\n",
    "        ax.set_xlabel('Models')\n",
    "        ax.set_ylabel(metrics[i])\n",
    "        ax.set_title(f'{metrics[i]} Comparison')\n",
    "        ax.set_xticks(index)\n",
    "        ax.set_xticklabels(models)\n",
    "\n",
    "    # Add legend outside the plot with adjusted parameters\n",
    "    handles = [\n",
    "        plt.Rectangle((0,0),1,1, color='lightgreen', alpha=0.6),\n",
    "        plt.Rectangle((0,0),1,1, color='green'),\n",
    "        plt.Rectangle((0,0),1,1, color='mediumseagreen'),  # Using mediumseagreen for Model 2 before tuning\n",
    "        plt.Rectangle((0,0),1,1, color='seagreen'),\n",
    "        plt.Rectangle((0,0),1,1, color='lightcoral', alpha=0.6),\n",
    "        plt.Rectangle((0,0),1,1, color='red')\n",
    "    ]\n",
    "    labels = [\n",
    "        'Model 1 - Before Tuning', 'Model 1 - After Tuning',\n",
    "        'Model 2 - Before Tuning', 'Model 2 - After Tuning',\n",
    "        'Model 3 - Before Tuning', 'Model 3 - After Tuning'\n",
    "    ]\n",
    "    fig.legend(handles=handles, labels=labels, loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=3, fontsize='large')  # Adjusted bbox_to_anchor and fontsize\n",
    "\n",
    "    # Adjust subplot spacing\n",
    "    plt.subplots_adjust(bottom=0.15)  # Increase bottom padding for the legend\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to plot side-by-side comparison of confusion matrix components\n",
    "def plot_confusion_matrix_comparison(confusion_matrix_before, confusion_matrix_after):\n",
    "    metrics_names = ['TN', 'FP', 'FN', 'TP']\n",
    "    num_metrics = len(metrics_names)\n",
    "    fig, axes = plt.subplots(1, num_metrics, figsize=(18, 6))\n",
    "\n",
    "    for i, metric_name in enumerate(metrics_names):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        index = np.arange(len(models))\n",
    "        bar_width = 0.35\n",
    "        \n",
    "        before_vals = confusion_matrix_before[:, i]\n",
    "        after_vals = confusion_matrix_after[:, i]\n",
    "        \n",
    "        for j, model in enumerate(models):\n",
    "            ax.bar(index[j] - bar_width/2, before_vals[j], bar_width, \n",
    "                   label='Before Tuning', color=colors[model]['Before Tuning'], alpha=0.6)\n",
    "            ax.bar(index[j] + bar_width/2, after_vals[j], bar_width, \n",
    "                   label='After Tuning', color=colors[model]['After Tuning'], alpha=1.0)\n",
    "\n",
    "            ax.text(index[j] - bar_width/2, before_vals[j] + 0.0005, f'{before_vals[j]:.0f}', ha='center', va='bottom')\n",
    "            ax.text(index[j] + bar_width/2, after_vals[j] + 0.0005, f'{after_vals[j]:.0f}', ha='center', va='bottom')\n",
    "\n",
    "        ax.set_xlabel('Models')\n",
    "        ax.set_ylabel(metric_name)\n",
    "        ax.set_title(f'{metric_name} Comparison')\n",
    "        ax.set_xticks(index)\n",
    "        ax.set_xticklabels(models)\n",
    "\n",
    "    # Add legend outside the plot with adjusted parameters\n",
    "    handles = [\n",
    "        plt.Rectangle((0,0),1,1, color='lightgreen', alpha=0.6),\n",
    "        plt.Rectangle((0,0),1,1, color='green'),\n",
    "        plt.Rectangle((0,0),1,1, color='mediumseagreen'),  # Using mediumseagreen for Model 2 before tuning\n",
    "        plt.Rectangle((0,0),1,1, color='lightcoral', alpha=0.6),\n",
    "        plt.Rectangle((0,0),1,1, color='red')\n",
    "    ]\n",
    "    labels = [\n",
    "        'Model 1 - Before Tuning', 'Model 1 - After Tuning',\n",
    "        'Model 2 - Before Tuning', 'Model 2 - After Tuning',\n",
    "        'Model 3 - Before Tuning', 'Model 3 - After Tuning'\n",
    "    ]\n",
    "   \n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plotting metrics comparison\n",
    "plot_metrics_comparison(metrics, metrics_values_before[:, :5], metrics_values_after[:, :5])\n",
    "\n",
    "# Plotting confusion matrix components comparison\n",
    "plot_confusion_matrix_comparison(confusion_matrix_before, confusion_matrix_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81f6341",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c4333",
   "metadata": {},
   "source": [
    "#### after checking the results, visisted again CRISP DM , varified the previous decisions and create new feature from the existing feature - Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c38c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame and 'feature592' is the column with date-time values\n",
    "\n",
    "# Convert the 'feature592' column to datetime format\n",
    "merged_df['feature592'] = pd.to_datetime(merged_df['feature592'], format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Extract date and time components\n",
    "merged_df['year'] = merged_df['feature592'].dt.year\n",
    "merged_df['month'] = merged_df['feature592'].dt.month\n",
    "merged_df['day'] = merged_df['feature592'].dt.day\n",
    "merged_df['hour'] = merged_df['feature592'].dt.hour\n",
    "merged_df['minute'] = merged_df['feature592'].dt.minute\n",
    "merged_df['second'] = merged_df['feature592'].dt.second\n",
    "\n",
    "# Day of the week (0=Monday, 6=Sunday)\n",
    "merged_df['day_of_week'] = merged_df['feature592'].dt.dayofweek\n",
    "\n",
    "# Part of the day (morning, afternoon, evening, night)\n",
    "def part_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 1  # morning\n",
    "    elif 12 <= hour < 17:\n",
    "        return 2  # afternoon\n",
    "    elif 17 <= hour < 21:\n",
    "        return 3  # evening\n",
    "    else:\n",
    "        return 0  # night\n",
    "\n",
    "merged_df['part_of_day'] = merged_df['hour'].apply(part_of_day)\n",
    "\n",
    "# Elapsed time since first entry in minutes\n",
    "merged_df['elapsed_time'] = (merged_df['feature592'] - merged_df['feature592'].min()).dt.total_seconds() / 60\n",
    "\n",
    "# Seasonal features\n",
    "merged_df['quarter'] = merged_df['feature592'].dt.quarter\n",
    "\n",
    "# Flag for weekend\n",
    "merged_df['is_weekend'] = (merged_df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Time differences between consecutive entries in minutes\n",
    "merged_df['time_diff'] = merged_df['feature592'].diff().dt.total_seconds() / 60\n",
    "merged_df['time_diff'].fillna(0, inplace=True)\n",
    "\n",
    "# Display the DataFrame with new features\n",
    "print(merged_df.head())\n",
    "\n",
    "# Drop the original datetime column if not needed\n",
    "merged_df.drop('feature592', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e44eb04",
   "metadata": {},
   "source": [
    "## Model building after feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604f009a",
   "metadata": {},
   "source": [
    "## MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb83c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame with the relevant data\n",
    "X = merged_df.drop('feature591', axis=1)  # Replace 'feature591' with your actual target column name\n",
    "y = merged_df['feature591']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Boruta Feature Selection\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "boruta_selector = BorutaPy(rf_clf, n_estimators='auto', verbose=0, random_state=42)  # Set verbose to 0\n",
    "boruta_selector.fit(X_train.values, y_train)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X_train.columns[boruta_selector.support_].tolist()\n",
    "print(\"Selected Features: \", selected_features)\n",
    "\n",
    "# Update X_train and X_test with selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# SMOTE Balancing on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_clf.fit(X_train_scaled, y_train_res)\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate Loss Cost\n",
    "cost_fp = 1000\n",
    "cost_fn = 5000\n",
    "loss_cost = cost_fp * fp + cost_fn * fn\n",
    "\n",
    "print(\"Accuracy Score: \", accuracy)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall (Sensitivity): \", recall)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"Loss Cost: \", loss_cost)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9e3757",
   "metadata": {},
   "source": [
    "# Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e92de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame with the relevant data\n",
    "X = merged_df.drop('feature591', axis=1)  # Replace 'feature591' with your actual target column name\n",
    "y = merged_df['feature591']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Boruta Feature Selection\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "boruta_selector = BorutaPy(rf_clf, n_estimators='auto', verbose=0, random_state=42)  # Set verbose to 0\n",
    "boruta_selector.fit(X_train.values, y_train)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X_train.columns[boruta_selector.support_].tolist()\n",
    "print(\"Selected Features: \", selected_features)\n",
    "\n",
    "# Update X_train and X_test with selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# SMOTE Balancing on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Define parameter grid for RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning without cross-validation\n",
    "best_f1_score = -1\n",
    "best_params = None\n",
    "\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for min_samples_split in param_grid['min_samples_split']:\n",
    "            for min_samples_leaf in param_grid['min_samples_leaf']:\n",
    "                # Train RandomForestClassifier with current parameters\n",
    "                rf_clf = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                                max_depth=max_depth, \n",
    "                                                min_samples_split=min_samples_split, \n",
    "                                                min_samples_leaf=min_samples_leaf,\n",
    "                                                n_jobs=-1,\n",
    "                                                class_weight='balanced',\n",
    "                                                random_state=42)\n",
    "                \n",
    "                rf_clf.fit(X_train_scaled, y_train_res)\n",
    "                \n",
    "                # Predict on test data\n",
    "                y_pred = rf_clf.predict(X_test_scaled)\n",
    "                \n",
    "                # Calculate F1 score\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                \n",
    "                # Check if current model is better than previous ones\n",
    "                if f1 > best_f1_score:\n",
    "                    best_f1_score = f1\n",
    "                    best_params = {\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'max_depth': max_depth,\n",
    "                        'min_samples_split': min_samples_split,\n",
    "                        'min_samples_leaf': min_samples_leaf\n",
    "                    }\n",
    "\n",
    "# Train final RandomForestClassifier with best parameters\n",
    "best_rf_clf = RandomForestClassifier(n_estimators=best_params['n_estimators'], \n",
    "                                     max_depth=best_params['max_depth'], \n",
    "                                     min_samples_split=best_params['min_samples_split'], \n",
    "                                     min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                     n_jobs=-1,\n",
    "                                     class_weight='balanced',\n",
    "                                     random_state=42)\n",
    "\n",
    "best_rf_clf.fit(X_train_scaled, y_train_res)\n",
    "\n",
    "# Predict on test data with the best model\n",
    "y_pred = best_rf_clf.predict(X_test_scaled)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate Loss Cost\n",
    "cost_fp = 1000\n",
    "cost_fn = 5000\n",
    "loss_cost = cost_fp * fp + cost_fn * fn\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best F1 Weighted Score:\", best_f1_score)\n",
    "\n",
    "print(\"Accuracy Score: \", accuracy)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall (Sensitivity): \", recall)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"Loss Cost: \", loss_cost)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d103c273",
   "metadata": {},
   "source": [
    "# K fold "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a7014b",
   "metadata": {},
   "source": [
    "#### K-Fold Cross-Validation is a technique used in machine learning to evaluate the performance of a model and ensure its generalizability to an independent dataset. It reduces bias and variance, makes efficient use of data, helps prevent overfitting, and provides a consistent basis for model comparison. By systematically training and testing models on different subsets of data, it ensures a thorough and reliable performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7561b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame with the relevant data\n",
    "X = merged_df.drop('feature591', axis=1)  # Replace 'feature591' with your actual target column name\n",
    "y = merged_df['feature591']\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics across folds\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "specificity_scores = []\n",
    "loss_costs = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(X, y)):\n",
    "    print(f\"\\nFold {fold_idx + 1}:\")\n",
    "\n",
    "    # Split data into training and testing sets for this fold\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Boruta Feature Selection\n",
    "    rf_clf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "    boruta_selector = BorutaPy(rf_clf, n_estimators='auto', verbose=0, random_state=42)  # Set verbose to 0\n",
    "    boruta_selector.fit(X_train.values, y_train)\n",
    "\n",
    "    # Selected features\n",
    "    selected_features = X_train.columns[boruta_selector.support_].tolist()\n",
    "    print(\"Selected Features: \", selected_features)\n",
    "\n",
    "    # Update X_train and X_test with selected features\n",
    "    X_train_selected = X_train[selected_features]\n",
    "    X_test_selected = X_test[selected_features]\n",
    "\n",
    "    # SMOTE Balancing on training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "    # Min-Max Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "    # Define parameter grid for RandomForestClassifier\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [3, 5, 7, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10, 15],\n",
    "        'min_samples_leaf': [1, 2, 4, 6]\n",
    "    }\n",
    "\n",
    "    # Perform hyperparameter tuning without cross-validation\n",
    "    best_f1_score = -1\n",
    "    best_params = None\n",
    "\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for min_samples_split in param_grid['min_samples_split']:\n",
    "                for min_samples_leaf in param_grid['min_samples_leaf']:\n",
    "                    # Train RandomForestClassifier with current parameters\n",
    "                    rf_clf = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                                    max_depth=max_depth,\n",
    "                                                    min_samples_split=min_samples_split,\n",
    "                                                    min_samples_leaf=min_samples_leaf,\n",
    "                                                    n_jobs=-1,\n",
    "                                                    class_weight='balanced',\n",
    "                                                    random_state=42)\n",
    "\n",
    "                    rf_clf.fit(X_train_scaled, y_train_res)\n",
    "\n",
    "                    # Predict on test data\n",
    "                    y_pred = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "                    # Calculate F1 score\n",
    "                    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "                    # Check if current model is better than previous ones\n",
    "                    if f1 > best_f1_score:\n",
    "                        best_f1_score = f1\n",
    "                        best_params = {\n",
    "                            'n_estimators': n_estimators,\n",
    "                            'max_depth': max_depth,\n",
    "                            'min_samples_split': min_samples_split,\n",
    "                            'min_samples_leaf': min_samples_leaf\n",
    "                        }\n",
    "\n",
    "    # Train final RandomForestClassifier with best parameters\n",
    "    best_rf_clf = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
    "                                         max_depth=best_params['max_depth'],\n",
    "                                         min_samples_split=best_params['min_samples_split'],\n",
    "                                         min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                         n_jobs=-1,\n",
    "                                         class_weight='balanced',\n",
    "                                         random_state=42)\n",
    "\n",
    "    best_rf_clf.fit(X_train_scaled, y_train_res)\n",
    "\n",
    "    # Predict on test data with the best model\n",
    "    y_pred = best_rf_clf.predict(X_test_scaled)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    # Calculate Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Calculate Loss Cost\n",
    "    cost_fp = 1000\n",
    "    cost_fn = 5000\n",
    "    loss_cost = cost_fp * fp + cost_fn * fn\n",
    "\n",
    "    # Append scores to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    specificity_scores.append(specificity)\n",
    "    loss_costs.append(loss_cost)\n",
    "\n",
    "    # Print evaluation metrics for the fold\n",
    "    print(\"\\nEvaluation Metrics for Fold:\")\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best F1 Weighted Score:\", best_f1_score)\n",
    "    print(\"Accuracy Score: \", accuracy)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall (Sensitivity): \", recall)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    print(\"Loss Cost: \", loss_cost)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "\n",
    "# Print average scores across all folds\n",
    "print(\"\\nAverage Metrics Across All Folds:\")\n",
    "print(\"Average Accuracy Score: \", np.mean(accuracy_scores))\n",
    "print(\"Average F1 Score: \", np.mean(f1_scores))\n",
    "print(\"Average Precision: \", np.mean(precision_scores))\n",
    "print(\"Average Recall (Sensitivity): \", np.mean(recall_scores))\n",
    "print(\"Average Specificity: \", np.mean(specificity_scores))\n",
    "print(\"Average Loss Cost: \", np.mean(loss_costs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc77ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f61ff4e",
   "metadata": {},
   "source": [
    "# model bulding with boruta features "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b17b847",
   "metadata": {},
   "source": [
    "## model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e32fc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame with the selected features and target\n",
    "selected_features = ['feature17', 'feature41', 'feature60', 'feature66', 'feature342', 'feature427', 'feature442', 'feature563', 'elapsed_time']\n",
    "X = merged_df[selected_features]\n",
    "y = merged_df['feature591']  # Replace 'feature591' with your actual target column name\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# SMOTE Data Balancing\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42),\n",
    "    'SVM': SVC(probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "# Dictionary to store evaluation metrics\n",
    "metrics = {\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': [],\n",
    "    'ROC AUC': [],\n",
    "    'Loss Cost': [],\n",
    "    'Confusion Matrix': []\n",
    "}\n",
    "\n",
    "# Iterate over classifiers\n",
    "for clf_name, clf in classifiers.items():\n",
    "    # Train the model\n",
    "    clf.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test_scaled)\n",
    "    y_pred_prob = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_prob)\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    # Calculate loss cost\n",
    "    cost_fp = 1000\n",
    "    cost_fn = 5000\n",
    "    FP = conf_matrix[0, 1]\n",
    "    FN = conf_matrix[1, 0]\n",
    "    loss_cost = cost_fp * FP + cost_fn * FN\n",
    "    \n",
    "    # Store metrics\n",
    "    metrics['Accuracy'].append(accuracy)\n",
    "    metrics['Precision'].append(precision)\n",
    "    metrics['Recall'].append(recall)\n",
    "    metrics['F1 Score'].append(f1)\n",
    "    metrics['ROC AUC'].append(roc_auc)\n",
    "    metrics['Loss Cost'].append(loss_cost)\n",
    "    metrics['Confusion Matrix'].append(conf_matrix)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\"Classifier: {clf_name}\"\")\n",
    "    print(f\"\"Accuracy: {accuracy}\"\")\n",
    "    print(f\"\"Precision: {precision}\"\")\n",
    "    print(f\"\"Recall: {recall}\"\")\n",
    "    print(f\"\"F1 Score: {f1}\"\")\n",
    "    print(f\"\"ROC AUC: {roc_auc}\"\")\n",
    "    print(f\"\"Confusion Matrix:\\n{conf_matrix}\"\")\n",
    "    print(f\"\"Loss Cost: {loss_cost}\"\")\n",
    "    print(\"\"\\n\"\")\n",
    "\n",
    "# Plotting ROC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for clf_name, clf in classifiers.items():\n",
    "    y_pred_prob = clf.predict_proba(X_test_scaled)[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f'{clf_name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"\"lower right\"\")\n",
    "plt.show()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01668886",
   "metadata": {},
   "source": [
    "##  TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d70afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "\n",
    "# Define fixed set of features\n",
    "fixed_features = ['feature17', 'feature41', 'feature60', 'feature66', 'feature342', 'feature427', 'feature442', 'feature563', 'elapsed_time']\n",
    "\n",
    "# Select only the fixed features\n",
    "X_train_selected = X_train[fixed_features]\n",
    "X_test_selected = X_test[fixed_features]\n",
    "\n",
    "# SMOTE Balancing on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Define parameter grid for RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning without cross-validation\n",
    "best_f1_score = -1\n",
    "best_params = None\n",
    "\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for min_samples_split in param_grid['min_samples_split']:\n",
    "           for min_samples_leaf in param_grid['min_samples_leaf']:\n",
    "                # Train RandomForestClassifier with current parameters\n",
    "                rf_clf = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                                max_depth=max_depth, \n",
    "                                                min_samples_split=min_samples_split, \n",
    "                                                min_samples_leaf=min_samples_leaf,\n",
    "                                                n_jobs=-1,\n",
    "                                                class_weight='balanced',\n",
    "                                                random_state=42)\n",
    "                \n",
    "                rf_clf.fit(X_train_scaled, y_train_res)\n",
    "                \n",
    "                # Predict on test data\n",
    "                y_pred = rf_clf.predict(X_test_scaled)\n",
    "                \n",
    "                # Calculate F1 score\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                \n",
    "                # Check if current model is better than previous ones\n",
    "                if f1 > best_f1_score:\n",
    "                    best_f1_score = f1\n",
    "                    best_params = {\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'max_depth': max_depth,\n",
    "                        'min_samples_split': min_samples_split,\n",
    "                        'min_samples_leaf': min_samples_leaf\n",
    "                    }\n",
    "\n",
    "# Train final RandomForestClassifier with best parameters\n",
    "best_rf_clf = RandomForestClassifier(n_estimators=best_params['n_estimators'], \n",
    "                                     max_depth=best_params['max_depth'], \n",
    "                                     min_samples_split=best_params['min_samples_split'], \n",
    "                                     min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                     n_jobs=-1,\n",
    "                                     class_weight='balanced',\n",
    "                                     random_state=42)\n",
    "\n",
    "best_rf_clf.fit(X_train_scaled, y_train_res)\n",
    "\n",
    "# Predict on test data with the best model\n",
    "y_pred = best_rf_clf.predict(X_test_scaled)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate Loss Cost\n",
    "cost_fp = 1000\n",
    "cost_fn = 5000\n",
    "loss_cost = cost_fp * fp + cost_fn * fn\n",
    "\n",
    "print(\"\"Best Parameters:\"\", best_params)\n",
    "print(\"\"Best F1 Weighted Score:\"\", best_f1_score)\n",
    "\n",
    "print(\"\"Accuracy Score: \"\", accuracy)\n",
    "print(\"\"F1 Score: \"\", f1)\n",
    "print(\"\"Precision: \"\", precision)\n",
    "print(\"\"Recall (Sensitivity): \"\", recall)\n",
    "print(\"\"Specificity: \"\", specificity)\n",
    "print(\"\"Loss Cost: \"\", loss_cost)\n",
    "\n",
    "print(\"\"Confusion Matrix:\"\")\n",
    "print(conf_matrix)\n",
    "print(f\"\"True Positives (TP): {tp}\"\")\n",
    "print(f\"\"True Negatives (TN): {tn}\"\")\n",
    "print(f\"\"False Positives (FP): {fp}\"\")\n",
    "print(f\"\"False Negatives (FN): {fn}\"\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a86594",
   "metadata": {},
   "source": [
    "## K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69afed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics across folds\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "specificity_scores = []\n",
    "loss_costs = []\n",
    "\n",
    "# Assuming X_train, y_train, X_test, y_test are already defined\n",
    "X_train = X_train_cleaned[['feature17', 'feature41', 'feature60', 'feature66', 'feature342', 'feature427', 'feature442', 'feature563', 'elapsed_time']].copy()\n",
    "y_train = y_train.copy()\n",
    "X_test = X_test_cleaned[['feature17', 'feature41', 'feature60', 'feature66', 'feature342', 'feature427', 'feature442', 'feature563', 'elapsed_time']].copy()\n",
    "y_test = y_test.copy()\n",
    "\n",
    "# Perform KNN imputation within each fold\n",
    "for train_index, test_index in kfold.split(X_train, y_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "    \n",
    "    # KNN imputation on training fold\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    X_train_imputed = imputer.fit_transform(X_train_fold)\n",
    "    X_val_imputed = imputer.transform(X_val_fold)\n",
    "    \n",
    "    # SMOTE Balancing on training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train_imputed, y_train_fold)\n",
    "    \n",
    "    # Min-Max Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "    X_val_scaled = scaler.transform(X_val_imputed)\n",
    "    \n",
    "    # Define parameter grid for RandomForestClassifier\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [3, 5, 7, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10, 15],\n",
    "        'min_samples_leaf': [1, 2, 4, 6]\n",
    "    }\n",
    "    \n",
    "    # Perform hyperparameter tuning without cross-validation\n",
    "    best_f1_score = -1\n",
    "    best_params = None\n",
    "    \n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for min_samples_split in param_grid['min_samples_split']:\n",
    "                for min_samples_leaf in param_grid['min_samples_leaf']:\n",
    "                    # Train RandomForestClassifier with current parameters\n",
    "                    rf_clf = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                                    max_depth=max_depth,\n",
    "                                                    min_samples_split=min_samples_split,\n",
    "                                                    min_samples_leaf=min_samples_leaf,\n",
    "                                                    n_jobs=-1,\n",
    "                                                    class_weight='balanced',\n",
    "                                                    random_state=42)\n",
    "                    rf_clf.fit(X_train_scaled, y_train_res)\n",
    "                    \n",
    "                    # Predict on validation data\n",
    "                    y_pred = rf_clf.predict(X_val_scaled)\n",
    "                    \n",
    "                    # Calculate F1 score\n",
    "                    f1 = f1_score(y_val_fold, y_pred, average='weighted')\n",
    "                    \n",
    "                    # Check if current model is better than previous ones\n",
    "                    if f1 > best_f1_score:\n",
    "                        best_f1_score = f1\n",
    "                        best_params = {\n",
    "                            'n_estimators': n_estimators,\n",
    "                            'max_depth': max_depth,\n",
    "                            'min_samples_split': min_samples_split,\n",
    "                            'min_samples_leaf': min_samples_leaf\n",
    "                        }\n",
    "    \n",
    "    # Train final RandomForestClassifier with best parameters on full training fold\n",
    "    best_rf_clf = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
    "                                         max_depth=best_params['max_depth'],\n",
    "                                         min_samples_split=best_params['min_samples_split'],\n",
    "                                         min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                         n_jobs=-1,\n",
    "                                         class_weight='balanced',\n",
    "                                         random_state=42)\n",
    "    \n",
    "    best_rf_clf.fit(X_train_scaled, y_train_res)\n",
    "    \n",
    "    # Predict on test data with the best model\n",
    "    X_test_imputed = imputer.transform(X_test)  # Impute test data\n",
    "    X_test_scaled = scaler.transform(X_test_imputed)  # Scale test data\n",
    "    y_pred = best_rf_clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    # Calculate Loss Cost\n",
    "    cost_fp = 1000\n",
    "    cost_fn = 5000\n",
    "    loss_cost = cost_fp * fp + cost_fn * fn\n",
    "    \n",
    "    # Append scores to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    specificity_scores.append(specificity)\n",
    "    loss_costs.append(loss_cost)\n",
    "    \n",
    "    # Print evaluation metrics for the fold\n",
    "    print(\"\"\\nEvaluation Metrics for Fold:\"\")\n",
    "    print(\"\"Best Parameters:\"\", best_params)\n",
    "    print(\"\"Best F1 Weighted Score:\"\", best_f1_score)\n",
    "    print(\"\"Accuracy Score: \"\", accuracy)\n",
    "    print(\"\"F1 Score: \"\", f1)\n",
    "    print(\"\"Precision: \"\", precision)\n",
    "    print(\"\"Recall (Sensitivity): \"\", recall)\n",
    "    print(\"\"Specificity: \"\", specificity)\n",
    "    print(\"\"Loss Cost: \"\", loss_cost)\n",
    "    \n",
    "    print(\"\"Confusion Matrix:\"\")\n",
    "    print(conf_matrix)\n",
    "    print(f\"\"True Positives (TP): {tp}\"\")\n",
    "    print(f\"\"True Negatives (TN): {tn}\"\")\n",
    "    print(f\"\"False Positives (FP): {fp}\"\")\n",
    "    print(f\"\"False Negatives (FN): {fn}\"\")\n",
    "\n",
    "# Print average scores across all folds\n",
    "print(\"\"\\nAverage Metrics Across All Folds:\"\")\n",
    "print(\"\"Average Accuracy Score: \"\", np.mean(accuracy_scores))\n",
    "print(\"\"Average F1 Score: \"\", np.mean(f1_scores))\n",
    "print(\"\"Average Precision: \"\", np.mean(precision_scores))\n",
    "print(\"\"Average Recall (Sensitivity): \"\", np.mean(recall_scores))\n",
    "print(\"\"Average Specificity: \"\", np.mean(specificity_scores))\n",
    "print(\"\"Average Loss Cost: \"\", np.mean(loss_costs))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eafaebd",
   "metadata": {},
   "source": [
    "## Model 3 - before tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198dae10",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['feature17', 'feature41', 'feature60', 'feature66', 'feature342', 'feature427', 'feature442', 'feature563', 'elapsed_time']\n",
    "\n",
    "\n",
    "# SMOTE Balancing on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train GaussianNB without tuning\n",
    "nb_clf = GaussianNB()\n",
    "nb_clf.fit(X_train_scaled, y_train_res)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = nb_clf.predict(X_test_scaled)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate Loss Cost\n",
    "cost_fp = 1000\n",
    "cost_fn = 5000\n",
    "loss_cost = cost_fp * fp + cost_fn * fn\n",
    "\n",
    "print(\"\"\\nEvaluation Metrics:\"\")\n",
    "print(\"\"Accuracy Score: \"\", accuracy)\n",
    "print(\"\"F1 Score: \"\", f1)\n",
    "print(\"\"Precision: \"\", precision)\n",
    "print(\"\"Recall (Sensitivity): \"\", recall)\n",
    "print(\"\"Specificity: \"\", specificity)\n",
    "print(\"\"Loss Cost: \"\", loss_cost)\n",
    "\n",
    "print(\"\"\\nConfusion Matrix:\"\")\n",
    "print(conf_matrix)\n",
    "print(f\"\"True Positives (TP): {tp}\"\")\n",
    "print(f\"\"True Negatives (TN): {tn}\"\")\n",
    "print(f\"\"False Positives (FP): {fp}\"\")\n",
    "print(f\"\"False Negatives (FN): {fn}\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e027e2b",
   "metadata": {},
   "source": [
    "# Model 3 - after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe11136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# Selecting specific features identified by Boruta\n",
    "selected_features = ['feature60', 'feature65', 'feature66', 'feature342', 'feature351', 'feature478', 'feature540', 'feature563']\n",
    "\n",
    "\n",
    "# Update X_train and X_test with selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# SMOTE Balancing on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Define parameter grid for GaussianNB\n",
    "param_grid = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "}\n",
    "\n",
    "# Perform parameter tuning\n",
    "best_f1_score = -1\n",
    "best_params = None\n",
    "\n",
    "for var_smoothing in param_grid['var_smoothing']:\n",
    "    # Train GaussianNB with current parameters\n",
    "    nb_clf = GaussianNB(var_smoothing=var_smoothing)\n",
    "    nb_clf.fit(X_train_scaled, y_train_res)\n",
    "    \n",
    "    # Predict on test data\n",
    "    y_pred = nb_clf.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate F1 score\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Check if current model is better than previous ones\n",
    "    if f1 > best_f1_score:\n",
    "        best_f1_score = f1\n",
    "        best_params = {\n",
    "            'var_smoothing': var_smoothing\n",
    "        }\n",
    "\n",
    "# Train final GaussianNB with best parameters\n",
    "best_nb_clf = GaussianNB(var_smoothing=best_params['var_smoothing'])\n",
    "best_nb_clf.fit(X_train_scaled, y_train_res)\n",
    "\n",
    "# Predict on test data with the best model\n",
    "y_pred = best_nb_clf.predict(X_test_scaled)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate Loss Cost\n",
    "cost_fp = 1000\n",
    "cost_fn = 5000\n",
    "loss_cost = cost_fp * fp + cost_fn * fn\n",
    "\n",
    "print(\"\"Best Parameters:\"\", best_params)\n",
    "print(\"\"Best F1 Weighted Score:\"\", best_f1_score)\n",
    "\n",
    "print(\"\"Accuracy Score: \"\", accuracy)\n",
    "print(\"\"F1 Score: \"\", f1)\n",
    "print(\"\"Precision: \"\", precision)\n",
    "print(\"\"Recall (Sensitivity): \"\", recall)\n",
    "print(\"\"Specificity: \"\", specificity)\n",
    "print(\"\"Loss Cost: \"\", loss_cost)\n",
    "\n",
    "print(\"\"Confusion Matrix:\"\")\n",
    "print(conf_matrix)\n",
    "print(f\"\"True Positives (TP): {tp}\"\")\n",
    "print(f\"\"True Negatives (TN): {tn}\"\")\n",
    "print(f\"\"False Positives (FP): {fp}\"\")\n",
    "print(f\"\"False Negatives (FN): {fn}\"\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5499551d",
   "metadata": {},
   "source": [
    "# Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d265bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the values for Model 1, Model 2, and Model 3 after tuning\n",
    "models = ['Model 1', 'Model 2', 'Model 3']\n",
    "TP = np.array([9, 6, 10])\n",
    "TN = np.array([337, 358, 218])\n",
    "FP = np.array([29, 8, 148])\n",
    "FN = np.array([17, 20, 16])\n",
    "\n",
    "# Calculate rates\n",
    "total_positives = TP + FN\n",
    "total_negatives = TN + FP\n",
    "\n",
    "TP_rate = TP / total_positives\n",
    "FP_rate = FP / total_negatives\n",
    "FN_rate = FN / total_positives\n",
    "TN_rate = TN / total_negatives\n",
    "\n",
    "# Plotting the comparison graph\n",
    "rates = ['TP Rate', 'FP Rate', 'FN Rate', 'TN Rate']\n",
    "\n",
    "# Transpose the rates for plotting\n",
    "rate_values = np.array([TP_rate, FP_rate, FN_rate, TN_rate])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot each line separately for each model\n",
    "for i, model in enumerate(models):\n",
    "    plt.plot(rates, rate_values[:, i], marker='o', linestyle='-', label=model)\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Rates')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Comparison of TP, FP, FN, TN Rates After Tuning for Model 1, Model 2, and Model 3')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6017ba26",
   "metadata": {},
   "source": [
    "## k fold volatility graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c11aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss costs for each fold\n",
    "fold_loss_costs = [114000, 99000, 85000, 103000, 94000]\n",
    "\n",
    "# Calculate average loss cost\n",
    "avg_loss_cost = 99000\n",
    "\n",
    "# Labels for folds\n",
    "fold_labels = ['Fold 1', 'Fold 2', 'Fold 3', 'Fold 4', 'Fold 5']\n",
    "\n",
    "# Plotting the loss costs\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(fold_labels, fold_loss_costs, color='skyblue', label='Loss Cost per Fold')\n",
    "plt.axhline(y=avg_loss_cost, color='orange', linestyle='--', label='Average Loss Cost')\n",
    "plt.xlabel('Folds')\n",
    "plt.ylabel('Loss Cost')\n",
    "plt.title('Loss Cost Comparison Across Folds')\n",
    "plt.ylim(0, max(fold_loss_costs) * 1.2)  # Adjust ylim for better visualization\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Adding values above bars\n",
    "for i, v in enumerate(fold_loss_costs):\n",
    "    plt.text(i, v + 5000, str(v), ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Adding average value annotation\n",
    "plt.text(len(fold_loss_costs) - 0.5, avg_loss_cost + 5000, f'Avg: {avg_loss_cost}', ha='center', va='bottom', fontsize=10, color='orange')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b9ab66",
   "metadata": {},
   "source": [
    "## before tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c040e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, learning_curve, validation_curve\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame with the relevant data\n",
    "# Replace 'merged_df' with your actual DataFrame containing the data\n",
    "# merged_df = pd.read_csv('your_data.csv')  # Replace with your data loading code\n",
    "X = merged_df.drop('feature591', axis=1)  # Replace 'feature591' with your actual target column name\n",
    "y = merged_df['feature591']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Boruta Feature Selection\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "boruta_selector = BorutaPy(rf_clf, n_estimators='auto', verbose=0, random_state=42)  # Set verbose to 0\n",
    "boruta_selector.fit(X_train.values, y_train)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X_train.columns[boruta_selector.support_].tolist()\n",
    "print(\"Selected Features: \", selected_features)\n",
    "\n",
    "# Update X_train and X_test with selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# SMOTE Balancing on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Define parameter grid for RandomForestClassifier\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10, 15],\n",
    "    'min_samples_leaf': [1, 2, 4, 6]\n",
    "}\n",
    "\n",
    "# Perform hyperparameter tuning without cross-validation\n",
    "best_f1_score = -1\n",
    "best_params = None\n",
    "\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for max_depth in param_grid['max_depth']:\n",
    "        for min_samples_split in param_grid['min_samples_split']:\n",
    "            for min_samples_leaf in param_grid['min_samples_leaf']:\n",
    "                # Train RandomForestClassifier with current parameters\n",
    "                rf_clf = RandomForestClassifier(n_estimators=n_estimators, \n",
    "                                                max_depth=max_depth, \n",
    "                                                min_samples_split=min_samples_split, \n",
    "                                                min_samples_leaf=min_samples_leaf,\n",
    "                                                n_jobs=-1,\n",
    "                                                class_weight='balanced',\n",
    "                                                random_state=42)\n",
    "                \n",
    "                rf_clf.fit(X_train_scaled, y_train_res)\n",
    "                \n",
    "                # Predict on test data\n",
    "                y_pred = rf_clf.predict(X_test_scaled)\n",
    "                \n",
    "                # Calculate F1 score\n",
    "                f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "                \n",
    "                # Check if current model is better than previous ones\n",
    "                if f1 > best_f1_score:\n",
    "                    best_f1_score = f1\n",
    "                    best_params = {\n",
    "                        'n_estimators': n_estimators,\n",
    "                        'max_depth': max_depth,\n",
    "                        'min_samples_split': min_samples_split,\n",
    "                        'min_samples_leaf': min_samples_leaf\n",
    "                    }\n",
    "\n",
    "# Train final RandomForestClassifier with best parameters\n",
    "best_rf_clf = RandomForestClassifier(n_estimators=best_params['n_estimators'], \n",
    "                                     max_depth=best_params['max_depth'], \n",
    "                                     min_samples_split=best_params['min_samples_split'], \n",
    "                                     min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                     n_jobs=-1,\n",
    "                                     class_weight='balanced',\n",
    "                                     random_state=42)\n",
    "\n",
    "best_rf_clf.fit(X_train_scaled, y_train_res)\n",
    "\n",
    "# Predict on test data with the best model\n",
    "y_pred = best_rf_clf.predict(X_test_scaled)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate Loss Cost\n",
    "cost_fp = 1000\n",
    "cost_fn = 5000\n",
    "loss_cost = cost_fp * fp + cost_fn * fn\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best F1 Weighted Score:\", best_f1_score)\n",
    "\n",
    "print(\"Accuracy Score: \", accuracy)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall (Sensitivity): \", recall)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"Loss Cost: \", loss_cost)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "\n",
    "# Plotting the learning curve\n",
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, scoring='f1_weighted', n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=scoring, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "title = \"Learning Curves (RandomForestClassifier)\"\n",
    "plot_learning_curve(best_rf_clf, title, X_train_scaled, y_train_res, cv=5, scoring='f1_weighted')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the validation curve\n",
    "def plot_validation_curve(estimator, title, X, y, param_name, param_range, ylim=None, cv=None,\n",
    "                          scoring=\"f1_weighted\", n_jobs=-1):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        estimator, X, y, param_name=param_name, param_range=param_range,\n",
    "        cv=cv, scoring=scoring, n_jobs=n_jobs)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.semilogx(param_range, train_scores_mean, label=\"Training score\",\n",
    "                 color=\"darkorange\", lw=2)\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.2,\n",
    "                     color=\"darkorange\", lw=2)\n",
    "    plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\",\n",
    "                 color=\"navy\", lw=2)\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.2,\n",
    "                     color=\"navy\", lw=2)\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "title = \"Validation Curve (RandomForestClassifier)\"\n",
    "param_name = \"n_estimators\"\n",
    "param_range = [50, 100, 200, 300]  # Specify the range of n_estimators\n",
    "plot_validation_curve(best_rf_clf, title, X_train_scaled, y_train_res, param_name=param_name,\n",
    "                      param_range=param_range, cv=5, scoring=\"f1_weighted\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5576828",
   "metadata": {},
   "source": [
    "## after tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2674c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, learning_curve\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame with the relevant data\n",
    "X = merged_df.drop('feature591', axis=1)  # Replace 'feature591' with your actual target column name\n",
    "y = merged_df['feature591']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Boruta Feature Selection\n",
    "rf_clf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "boruta_selector = BorutaPy(rf_clf, n_estimators='auto', verbose=0, random_state=42)  # Set verbose to 0\n",
    "boruta_selector.fit(X_train.values, y_train)\n",
    "\n",
    "# Selected features\n",
    "selected_features = X_train.columns[boruta_selector.support_].tolist()\n",
    "print(\"Selected Features: \", selected_features)\n",
    "\n",
    "# Update X_train and X_test with selected features\n",
    "X_train_selected = X_train[selected_features]\n",
    "X_test_selected = X_test[selected_features]\n",
    "\n",
    "# SMOTE Balancing on training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "# Min-Max Scaling\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "# Train Random Forest Classifier\n",
    "rf_clf.fit(X_train_scaled, y_train_res)\n",
    "y_pred = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "# Calculate Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate Loss Cost\n",
    "cost_fp = 1000\n",
    "cost_fn = 5000\n",
    "loss_cost = cost_fp * fp + cost_fn * fn\n",
    "\n",
    "print(\"Accuracy Score: \", accuracy)\n",
    "print(\"F1 Score: \", f1)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall (Sensitivity): \", recall)\n",
    "print(\"Specificity: \", specificity)\n",
    "print(\"Loss Cost: \", loss_cost)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "\n",
    "# Learning Curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    rf_clf, X_train_scaled, y_train_res, cv=5, scoring='f1_weighted', n_jobs=-1,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10))\n",
    "\n",
    "# Calculate mean and standard deviation of training scores and test scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "test_scores_mean = np.mean(test_scores, axis=1)\n",
    "test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "# Plotting the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title(\"Learning Curve (RandomForestClassifier)\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.grid()\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d47cf1",
   "metadata": {},
   "source": [
    "## After K fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c663c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, learning_curve\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'merged_df' is your DataFrame with the relevant data\n",
    "X = merged_df.drop('feature591', axis=1)  # Replace 'feature591' with your actual target column name\n",
    "y = merged_df['feature591']\n",
    "\n",
    "# Initialize k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store evaluation metrics across folds\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "specificity_scores = []\n",
    "loss_costs = []\n",
    "\n",
    "# Initialize lists to store learning curve data\n",
    "train_sizes_all = []\n",
    "train_scores_mean_all = []\n",
    "train_scores_std_all = []\n",
    "test_scores_mean_all = []\n",
    "test_scores_std_all = []\n",
    "\n",
    "# Iterate over each fold\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(kfold.split(X, y)):\n",
    "    print(f\"\\nFold {fold_idx + 1}:\")\n",
    "\n",
    "    # Split data into training and testing sets for this fold\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Boruta Feature Selection\n",
    "    rf_clf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5, random_state=42)\n",
    "    boruta_selector = BorutaPy(rf_clf, n_estimators='auto', verbose=0, random_state=42)  # Set verbose to 0\n",
    "    boruta_selector.fit(X_train.values, y_train)\n",
    "\n",
    "    # Selected features\n",
    "    selected_features = X_train.columns[boruta_selector.support_].tolist()\n",
    "    print(\"Selected Features: \", selected_features)\n",
    "\n",
    "    # Update X_train and X_test with selected features\n",
    "    X_train_selected = X_train[selected_features]\n",
    "    X_test_selected = X_test[selected_features]\n",
    "\n",
    "    # SMOTE Balancing on training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_res, y_train_res = smote.fit_resample(X_train_selected, y_train)\n",
    "\n",
    "    # Min-Max Scaling\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_res)\n",
    "    X_test_scaled = scaler.transform(X_test_selected)\n",
    "\n",
    "    # Define parameter grid for RandomForestClassifier\n",
    "    param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [3, 5, 7, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10, 15],\n",
    "        'min_samples_leaf': [1, 2, 4, 6]\n",
    "    }\n",
    "\n",
    "    # Perform hyperparameter tuning without cross-validation\n",
    "    best_f1_score = -1\n",
    "    best_params = None\n",
    "\n",
    "    for n_estimators in param_grid['n_estimators']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            for min_samples_split in param_grid['min_samples_split']:\n",
    "                for min_samples_leaf in param_grid['min_samples_leaf']:\n",
    "                    # Train RandomForestClassifier with current parameters\n",
    "                    rf_clf = RandomForestClassifier(n_estimators=n_estimators,\n",
    "                                                    max_depth=max_depth,\n",
    "                                                    min_samples_split=min_samples_split,\n",
    "                                                    min_samples_leaf=min_samples_leaf,\n",
    "                                                    n_jobs=-1,\n",
    "                                                    class_weight='balanced',\n",
    "                                                    random_state=42)\n",
    "\n",
    "                    rf_clf.fit(X_train_scaled, y_train_res)\n",
    "\n",
    "                    # Predict on test data\n",
    "                    y_pred = rf_clf.predict(X_test_scaled)\n",
    "\n",
    "                    # Calculate F1 score\n",
    "                    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "                    # Check if current model is better than previous ones\n",
    "                    if f1 > best_f1_score:\n",
    "                        best_f1_score = f1\n",
    "                        best_params = {\n",
    "                            'n_estimators': n_estimators,\n",
    "                            'max_depth': max_depth,\n",
    "                            'min_samples_split': min_samples_split,\n",
    "                            'min_samples_leaf': min_samples_leaf\n",
    "                        }\n",
    "\n",
    "    # Train final RandomForestClassifier with best parameters\n",
    "    best_rf_clf = RandomForestClassifier(n_estimators=best_params['n_estimators'],\n",
    "                                         max_depth=best_params['max_depth'],\n",
    "                                         min_samples_split=best_params['min_samples_split'],\n",
    "                                         min_samples_leaf=best_params['min_samples_leaf'],\n",
    "                                         n_jobs=-1,\n",
    "                                         class_weight='balanced',\n",
    "                                         random_state=42)\n",
    "\n",
    "    best_rf_clf.fit(X_train_scaled, y_train_res)\n",
    "\n",
    "    # Predict on test data with the best model\n",
    "    y_pred = best_rf_clf.predict(X_test_scaled)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    # Calculate Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Calculate Loss Cost\n",
    "    cost_fp = 1000\n",
    "    cost_fn = 5000\n",
    "    loss_cost = cost_fp * fp + cost_fn * fn\n",
    "\n",
    "    # Append scores to lists\n",
    "    accuracy_scores.append(accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    precision_scores.append(precision)\n",
    "    recall_scores.append(recall)\n",
    "    specificity_scores.append(specificity)\n",
    "    loss_costs.append(loss_cost)\n",
    "\n",
    "    # Print evaluation metrics for the fold\n",
    "    print(\"\\nEvaluation Metrics for Fold:\")\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    print(\"Best F1 Weighted Score:\", best_f1_score)\n",
    "    print(\"Accuracy Score: \", accuracy)\n",
    "    print(\"F1 Score: \", f1)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall (Sensitivity): \", recall)\n",
    "    print(\"Specificity: \", specificity)\n",
    "    print(\"Loss Cost: \", loss_cost)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(f\"True Positives (TP): {tp}\")\n",
    "    print(f\"True Negatives (TN): {tn}\")\n",
    "    print(f\"False Positives (FP): {fp}\")\n",
    "    print(f\"False Negatives (FN): {fn}\")\n",
    "\n",
    "    # Calculate learning curve\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        best_rf_clf, X_train_scaled, y_train_res, cv=kfold, n_jobs=-1,\n",
    "        scoring='f1_weighted', train_sizes=np.linspace(.1, 1.0, 5),\n",
    "        return_times=True)\n",
    "\n",
    "    # Store learning curve data\n",
    "    train_sizes_all.append(train_sizes)\n",
    "    train_scores_mean_all.append(np.mean(train_scores, axis=1))\n",
    "    train_scores_std_all.append(np.std(train_scores, axis=1))\n",
    "    test_scores_mean_all.append(np.mean(test_scores, axis=1))\n",
    "    test_scores_std_all.append(np.std(test_scores, axis=1))\n",
    "\n",
    "# Print average scores across all folds\n",
    "print(\"\\nAverage Metrics Across All Folds:\")\n",
    "print(\"Average Accuracy Score: \", np.mean(accuracy_scores))\n",
    "print(\"Average F1 Score: \", np.mean(f1_scores))\n",
    "print(\"Average Precision: \", np.mean(precision_scores))\n",
    "print(\"Average Recall (Sensitivity): \", np.mean(recall_scores))\n",
    "print(\"Average Specificity: \", np.mean(specificity_scores))\n",
    "print(\"Average Loss Cost: \", np.mean(loss_costs))\n",
    "\n",
    "# Plot learning curve\n",
    "plt.figure()\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel(\"Training examples\")\n",
    "plt.ylabel(\"Score\")\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "train_scores_mean_all = np.array(train_scores_mean_all)\n",
    "train_scores_std_all = np.array(train_scores_std_all)\n",
    "test_scores_mean_all = np.array(test_scores_mean_all)\n",
    "test_scores_std_all = np.array(test_scores_std_all)\n",
    "\n",
    "plt.fill_between(train_sizes_all[0], train_scores_mean_all.mean(axis=0) - train_scores_std_all.mean(axis=0),\n",
    "                 train_scores_mean_all.mean(axis=0) + train_scores_std_all.mean(axis=0), alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes_all[0], test_scores_mean_all.mean(axis=0) - test_scores_std_all.mean(axis=0),\n",
    "                 test_scores_mean_all.mean(axis=0) + test_scores_std_all.mean(axis=0), alpha=0.1, color=\"g\")\n",
    "plt.plot(train_sizes_all[0], train_scores_mean_all.mean(axis=0), 'o-', color=\"r\",\n",
    "         label=\"Training score\")\n",
    "plt.plot(train_sizes_all[0], test_scores_mean_all.mean(axis=0), 'o-', color=\"g\",\n",
    "         label=\"Cross-validation score\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
